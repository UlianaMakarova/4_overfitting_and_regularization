{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Overfitting and Regularization","metadata":{"id":"57oHIczrFm68"}},{"cell_type":"markdown","source":"## Imports","metadata":{"id":"ltJHzh--Fm6_"}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt","metadata":{"ExecuteTime":{"end_time":"2022-02-16T07:39:25.890581Z","start_time":"2022-02-16T07:39:23.835921Z"},"id":"zh_Uf84JFm7A","execution":{"iopub.status.busy":"2022-03-28T07:01:34.637468Z","iopub.execute_input":"2022-03-28T07:01:34.637839Z","iopub.status.idle":"2022-03-28T07:01:34.782118Z","shell.execute_reply.started":"2022-03-28T07:01:34.637793Z","shell.execute_reply":"2022-03-28T07:01:34.781180Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"from sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import train_test_split, cross_validate\nfrom sklearn.preprocessing import PolynomialFeatures, StandardScaler\nfrom sklearn.linear_model import LinearRegression, Lasso, Ridge\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn import set_config","metadata":{"ExecuteTime":{"end_time":"2022-02-16T07:39:26.108031Z","start_time":"2022-02-16T07:39:25.925893Z"},"id":"38FANhVpFm7C","execution":{"iopub.status.busy":"2022-03-28T07:01:34.784465Z","iopub.execute_input":"2022-03-28T07:01:34.784961Z","iopub.status.idle":"2022-03-28T07:01:34.926226Z","shell.execute_reply.started":"2022-03-28T07:01:34.784927Z","shell.execute_reply":"2022-03-28T07:01:34.925270Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"set_config(display='diagram')","metadata":{"ExecuteTime":{"end_time":"2022-02-16T07:39:26.155183Z","start_time":"2022-02-16T07:39:26.141249Z"},"id":"Dgj99jtfFm7E","execution":{"iopub.status.busy":"2022-03-28T07:01:34.928630Z","iopub.execute_input":"2022-03-28T07:01:34.928854Z","iopub.status.idle":"2022-03-28T07:01:34.933972Z","shell.execute_reply.started":"2022-03-28T07:01:34.928827Z","shell.execute_reply":"2022-03-28T07:01:34.933010Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"## Settings","metadata":{"id":"X6o8fzTtFm7F"}},{"cell_type":"code","source":"SEED = 42\nRANGE = (-5, 5)\nN_SAMPLES = 50\nDEGREES = np.linspace(0, 15, 1 + 15, dtype=int)\nALPHAS = np.linspace(0, 0.5, 1 + 40)","metadata":{"ExecuteTime":{"end_time":"2022-02-16T07:39:26.201661Z","start_time":"2022-02-16T07:39:26.189181Z"},"id":"6D17-LAMFm7G","execution":{"iopub.status.busy":"2022-03-28T07:01:34.954215Z","iopub.execute_input":"2022-03-28T07:01:34.955170Z","iopub.status.idle":"2022-03-28T07:01:34.961689Z","shell.execute_reply.started":"2022-03-28T07:01:34.955024Z","shell.execute_reply":"2022-03-28T07:01:34.961008Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"## Part 1: Underfitting vs. overfitting","metadata":{"id":"NYbabdyBFm7H"}},{"cell_type":"markdown","source":"### Generate samples","metadata":{"id":"NoBHaja_Fm7I"}},{"cell_type":"markdown","source":"Let's pick a target function $ f(x) = 2\\cdot x + 10\\cdot sin(x) $ and generate some noisy samples to learn from.","metadata":{"ExecuteTime":{"end_time":"2020-11-09T09:04:09.904994Z","start_time":"2020-11-09T09:04:09.896444Z"},"id":"pHVooi1MFm7J"}},{"cell_type":"code","source":"def target_function(x):\n    return 2 * x + 10 * np.sin(x)\n\ndef generate_samples():\n    \"\"\"Generate noisy samples.\"\"\"\n    np.random.seed(SEED)\n    x = np.random.uniform(*RANGE, size=N_SAMPLES)\n    y = target_function(x) + np.random.normal(scale=4, size=N_SAMPLES)\n    return x.reshape(-1, 1), y\n\nX, y = generate_samples()","metadata":{"ExecuteTime":{"end_time":"2022-02-16T07:39:26.433239Z","start_time":"2022-02-16T07:39:26.412501Z"},"id":"opI1jaZoFm7K","execution":{"iopub.status.busy":"2022-03-28T07:01:35.044296Z","iopub.execute_input":"2022-03-28T07:01:35.044721Z","iopub.status.idle":"2022-03-28T07:01:35.052682Z","shell.execute_reply.started":"2022-03-28T07:01:35.044691Z","shell.execute_reply":"2022-03-28T07:01:35.052017Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"### Plot samples","metadata":{"id":"sS6SKtVtFm7M"}},{"cell_type":"code","source":"def plot_scatter(x, y, title=None, label='Noisy samples'):\n    plt.scatter(x, y, label=label)\n    plt.xlabel('x')\n    plt.ylabel('y')\n    plt.grid(True)\n    plt.title(title)\n    plt.legend(loc='lower right')\n\nplot_scatter(X, y)\n","metadata":{"ExecuteTime":{"end_time":"2022-02-16T07:39:27.822431Z","start_time":"2022-02-16T07:39:27.696426Z"},"id":"5YGLVTBCFm7N","outputId":"d61276f7-a44b-45d3-bbf9-8d31b16635a6","execution":{"iopub.status.busy":"2022-03-28T07:01:35.121676Z","iopub.execute_input":"2022-03-28T07:01:35.122065Z","iopub.status.idle":"2022-03-28T07:01:35.372063Z","shell.execute_reply.started":"2022-03-28T07:01:35.122034Z","shell.execute_reply":"2022-03-28T07:01:35.371237Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"### Split","metadata":{"id":"aduHf6u4Fm7O"}},{"cell_type":"code","source":"X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.3, random_state=SEED)\n\nplot_scatter(X_train, y_train, label='Training set')\nplot_scatter(X_valid, y_valid, label='Validation set')","metadata":{"ExecuteTime":{"end_time":"2022-02-16T07:39:29.210072Z","start_time":"2022-02-16T07:39:29.093175Z"},"id":"wErymr7TFm7P","outputId":"aac84695-2f4e-4953-aa2c-cc564fb49023","execution":{"iopub.status.busy":"2022-03-28T07:01:35.373673Z","iopub.execute_input":"2022-03-28T07:01:35.373922Z","iopub.status.idle":"2022-03-28T07:01:35.597715Z","shell.execute_reply.started":"2022-03-28T07:01:35.373891Z","shell.execute_reply":"2022-03-28T07:01:35.596719Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"y_train","metadata":{"ExecuteTime":{"end_time":"2022-02-16T07:39:33.496625Z","start_time":"2022-02-16T07:39:33.479851Z"},"id":"BacqE9YRFm7P","outputId":"70e69aa1-e3f9-41d0-c11a-fab5bcedf528","execution":{"iopub.status.busy":"2022-03-28T07:01:35.598976Z","iopub.execute_input":"2022-03-28T07:01:35.599275Z","iopub.status.idle":"2022-03-28T07:01:35.606582Z","shell.execute_reply.started":"2022-03-28T07:01:35.599235Z","shell.execute_reply":"2022-03-28T07:01:35.605635Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"### Model","metadata":{"id":"Yo-PUOOnFm7Q"}},{"cell_type":"markdown","source":"Let's try to approximate our target function $ f(x) = 2\\cdot x + 10\\cdot sin(x) $ with polynomials of different degree. \n\nA polynomial of degree $n$ has the form:\n$ h(x) = w_0 + w_1\\cdot x + w_2\\cdot x^2 +\\ldots + w_n\\cdot x^n $.\n\n$x^i$ values could easily be generated by `PolynomialFeatures`, while $w_i$ are the unknown paramaters to be estimated using `LinearRegression`.","metadata":{"id":"zXaoIFisFm7S"}},{"cell_type":"code","source":"PolynomialFeatures(degree=4, include_bias=False).fit_transform(X=[\n    [1],\n    [3],\n    [4],\n])\n","metadata":{"ExecuteTime":{"end_time":"2022-02-16T07:39:34.647226Z","start_time":"2022-02-16T07:39:34.63954Z"},"id":"ZYNgwxnvFm7S","outputId":"b4e115d2-801d-45ed-e6ee-be5c33ab9b47","execution":{"iopub.status.busy":"2022-03-28T07:01:35.608682Z","iopub.execute_input":"2022-03-28T07:01:35.608990Z","iopub.status.idle":"2022-03-28T07:01:35.622163Z","shell.execute_reply.started":"2022-03-28T07:01:35.608948Z","shell.execute_reply":"2022-03-28T07:01:35.621387Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"def make_model(degree, alpha=0, penalty=None):\n    # linear regression\n    if alpha == 0:\n        regressor = LinearRegression()\n    # lasso regression\",\n    elif penalty == 'L1':\n        regressor = Lasso(alpha=alpha, random_state=SEED, max_iter=50000)\n    # ridge regression\",\n    elif penalty == 'L2':\n        regressor = Ridge(alpha=alpha, random_state=SEED, max_iter=50000) \n    \n    \n    return Pipeline([\n        ('pol', PolynomialFeatures(degree, include_bias=(degree == 0))),\n        ('sca', StandardScaler()),\n        ('reg', regressor)\n    ])\n\ndisplay(make_model(2))\ndisplay(make_model(2, penalty='L1', alpha=0.1))\ndisplay(make_model(2, penalty='L2', alpha=0.1))","metadata":{"ExecuteTime":{"end_time":"2022-02-16T07:40:49.993908Z","start_time":"2022-02-16T07:40:49.95934Z"},"id":"iDmw0hkJFm7T","outputId":"88f19040-8f35-487e-d198-56216b56c510","execution":{"iopub.status.busy":"2022-03-28T07:01:35.623072Z","iopub.execute_input":"2022-03-28T07:01:35.623286Z","iopub.status.idle":"2022-03-28T07:01:35.657822Z","shell.execute_reply.started":"2022-03-28T07:01:35.623259Z","shell.execute_reply":"2022-03-28T07:01:35.656857Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"### Fit","metadata":{"id":"qBWLs5_WFm7T"}},{"cell_type":"markdown","source":"Let's fit a model and plot the hypothesis it learns:","metadata":{"id":"2e9sYayaFm7U"}},{"cell_type":"code","source":"def plot_fit(model):\n    degree = model['pol'].degree\n    X_range = np.linspace(*RANGE, 1000).reshape(-1, 1)\n    y_pred = model.predict(X_range)\n    plot_scatter(X_train, y_train, label='Training sample')\n    plot_scatter(X_valid, y_valid, label='Validation sample')\n    plt.plot(X_range, target_function(X_range), c='green', alpha=0.2, lw=5, label='Target function')\n    plt.plot(X_range, y_pred, c='red', label='Hypothesis')\n    plt.ylim((min(y) - 3, max(y) + 3))\n    plt.legend(loc='best')    \n    plt.title(f'Polynomial approximation: degree={degree}')\n    plt.show()\n\nplot_fit(make_model(degree=2).fit(X_train, y_train))","metadata":{"ExecuteTime":{"end_time":"2022-02-16T07:40:51.426785Z","start_time":"2022-02-16T07:40:51.20983Z"},"id":"jC8VPvg_Fm7U","outputId":"f6cb91a4-9f00-4d7e-829e-1cfab85bc4a6","execution":{"iopub.status.busy":"2022-03-28T07:01:35.659228Z","iopub.execute_input":"2022-03-28T07:01:35.659780Z","iopub.status.idle":"2022-03-28T07:01:35.912925Z","shell.execute_reply.started":"2022-03-28T07:01:35.659733Z","shell.execute_reply":"2022-03-28T07:01:35.912062Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"### From underfitting to overfitting","metadata":{"id":"SCFYtnxwFm7V"}},{"cell_type":"markdown","source":"We can investigate the shape of the fitted curve for different values of `degree`:","metadata":{"ExecuteTime":{"end_time":"2020-11-09T11:15:24.323458Z","start_time":"2020-11-09T11:15:24.318089Z"},"id":"i2jduhA1Fm7V"}},{"cell_type":"code","source":"for degree in [0, 1, 2, 3, 4, 5, 10, 15, 20]:\n    plot_fit(make_model(degree).fit(X_train, y_train))","metadata":{"ExecuteTime":{"end_time":"2022-02-16T07:40:54.236522Z","start_time":"2022-02-16T07:40:52.748262Z"},"id":"hs_Vg94NFm7W","outputId":"f97005e7-5a36-4042-98de-e8a4a7415632","execution":{"iopub.status.busy":"2022-03-28T07:01:35.915938Z","iopub.execute_input":"2022-03-28T07:01:35.916287Z","iopub.status.idle":"2022-03-28T07:01:38.316955Z","shell.execute_reply.started":"2022-03-28T07:01:35.916213Z","shell.execute_reply":"2022-03-28T07:01:38.314927Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"### Fitting graph","metadata":{"id":"wy8YP1UaFm7W"}},{"cell_type":"markdown","source":"In the next step we calculate the training and the validation error for each `degree` and plot them in a single graph. The resulting graph is called the fitting graph.","metadata":{"id":"nhCr-Hc5Fm7W"}},{"cell_type":"code","source":"def rmse(y_true, y_pred):\n    return np.sqrt(mean_squared_error(y_true, y_pred))\n\ndef plot_fitting_graph(x, metric_train, metric_valid, xlabel, ylabel, \n                       custom_metric=None, custom_label='', custom_scale='log', title='Fitting graph'):\n    plt.figure(figsize=(9, 4.5))\n    plt.plot(x, metric_train, label='Training')\n    plt.plot(x, metric_valid, color='C1', label='Validation')\n    plt.axvline(x[np.argmin(metric_valid)], color='C1', lw=10, alpha=0.2)\n    plt.title(title)\n    plt.xlabel(xlabel)\n    plt.ylabel(ylabel)\n    plt.grid(True)\n    plt.xticks(x, rotation='vertical')\n    plt.legend(loc='center left')        \n    if custom_metric:\n        plt.twinx()\n        plt.yscale(custom_scale)\n        plt.plot(x, custom_metric, alpha=0.2, lw=4, ls='dotted', color='black', label=custom_label) \n        plt.legend(loc='center right')         \n    plt.show()\n    \nrmse_train, rmse_valid = [], []\nfor degree in DEGREES:\n    reg = make_model(degree).fit(X_train, y_train)\n    rmse_train.append(rmse(reg.predict(X_train), y_train))\n    rmse_valid.append(rmse(reg.predict(X_valid), y_valid))\n    \nplot_fitting_graph(DEGREES, rmse_train, rmse_valid, xlabel='Complexity (degree)', ylabel='Error (RMSE)', \n                   title='Least squares polynomial regression')","metadata":{"ExecuteTime":{"end_time":"2021-12-15T20:32:47.308471Z","start_time":"2021-12-15T20:32:47.036535Z"},"id":"DdXzPB1yFm7W","outputId":"7b2291d1-ed7c-4c67-d75f-9db033ce8583","execution":{"iopub.status.busy":"2022-03-28T07:01:38.318162Z","iopub.execute_input":"2022-03-28T07:01:38.318428Z","iopub.status.idle":"2022-03-28T07:01:38.609685Z","shell.execute_reply.started":"2022-03-28T07:01:38.318395Z","shell.execute_reply":"2022-03-28T07:01:38.608795Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"### Sweet spot","metadata":{"id":"n3tK-5MDFm7X"}},{"cell_type":"markdown","source":"What is the optimal `degree` to go with?","metadata":{"id":"GrfnVaQeFm7X"}},{"cell_type":"code","source":"DEGREES[np.argmin(rmse_valid)]","metadata":{"ExecuteTime":{"end_time":"2021-12-15T20:32:47.324492Z","start_time":"2021-12-15T20:32:47.308471Z"},"id":"tb_JDJhnFm7X","outputId":"011de964-9ca1-47af-84db-b455bfdde37d","execution":{"iopub.status.busy":"2022-03-28T07:01:38.611265Z","iopub.execute_input":"2022-03-28T07:01:38.611856Z","iopub.status.idle":"2022-03-28T07:01:38.618869Z","shell.execute_reply.started":"2022-03-28T07:01:38.611810Z","shell.execute_reply":"2022-03-28T07:01:38.617899Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"### Cross-validation","metadata":{"id":"MB-nw8qeFm7Y"}},{"cell_type":"markdown","source":"Ideally, we would choose the the model parameters such that we have the best model performance. However, we want to make sure that we really have the best validation performance. When we do `train_test_split` we randomly split the data into two parts. What could happen is that we got lucky and split the data such that it favours the validation error. This is especially dangerous if we are dealing with small datasets. One way to check if that's the case is to run the experiment several times for different, random splits. However, there is an even more systematic way of doing this: [cross-validation](https://scikit-learn.org/stable/modules/cross_validation.html).","metadata":{"id":"_Lhn-3VNFm7Y"}},{"cell_type":"markdown","source":"<img src=\"https://scikit-learn.org/stable/_images/grid_search_cross_validation.png\" width=50% />","metadata":{"id":"vWdUUAK9Fm7Z"}},{"cell_type":"code","source":"rmse_train, rmse_valid = [], []\nfor degree in DEGREES:\n    results = cross_validate(make_model(degree), \n                             X, y, cv=5,\n                             return_train_score=True,\n                             scoring='neg_root_mean_squared_error')\n    rmse_train.append(-np.mean(results['train_score']))\n    rmse_valid.append(-np.mean(results['test_score']))\n    \nplot_fitting_graph(DEGREES, rmse_train, rmse_valid, xlabel='Complexity (degree)', ylabel='Error (RMSE)',\n                   title='Least squares polynomial regression')","metadata":{"ExecuteTime":{"end_time":"2021-12-15T20:32:47.788153Z","start_time":"2021-12-15T20:32:47.324492Z"},"id":"aMVRzMcUFm7a","outputId":"1b03bbbe-5f87-44ec-86f3-7df85cda3802","execution":{"iopub.status.busy":"2022-03-28T07:01:38.621503Z","iopub.execute_input":"2022-03-28T07:01:38.621824Z","iopub.status.idle":"2022-03-28T07:01:39.094685Z","shell.execute_reply.started":"2022-03-28T07:01:38.621781Z","shell.execute_reply":"2022-03-28T07:01:39.094021Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":"### Model coefficients","metadata":{"id":"pixxFf3FFm7b"}},{"cell_type":"markdown","source":"Let's inspect our regression model coefficients:","metadata":{"ExecuteTime":{"end_time":"2020-11-05T16:26:33.200639Z","start_time":"2020-11-05T16:26:33.197656Z"},"id":"lnIce5Q_Fm7c"}},{"cell_type":"code","source":"(make_model(degree=1).fit(X_train, y_train)['reg'].coef_,\n make_model(degree=2).fit(X_train, y_train)['reg'].coef_,\n make_model(degree=5).fit(X_train, y_train)['reg'].coef_,\n make_model(degree=10).fit(X_train, y_train)['reg'].coef_)","metadata":{"ExecuteTime":{"end_time":"2021-12-15T20:32:47.820142Z","start_time":"2021-12-15T20:32:47.788153Z"},"id":"JDJWBtJiFm7c","outputId":"0ea211eb-8149-4290-c957-1b883af41e72","execution":{"iopub.status.busy":"2022-03-28T07:01:39.095583Z","iopub.execute_input":"2022-03-28T07:01:39.096087Z","iopub.status.idle":"2022-03-28T07:01:39.108892Z","shell.execute_reply.started":"2022-03-28T07:01:39.096054Z","shell.execute_reply":"2022-03-28T07:01:39.108275Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":"Hmm... it looks like high degree polynomials are coming with much bigger regression coefficients. \n\nWe are going to plot the mean absolute value of $w_i$ as a function of degree to reveal the relationship:","metadata":{"id":"7YcTRbhFFm7d"}},{"cell_type":"code","source":"rmse_train, rmse_valid, avg_coef = [], [], []\nfor degree in DEGREES:\n    results = cross_validate(make_model(degree),\n                             X, y, cv=5,\n                             return_train_score=True, return_estimator=True,\n                             scoring='neg_root_mean_squared_error')\n    rmse_train.append(-np.mean(results['train_score']))\n    rmse_valid.append(-np.mean(results['test_score']))        \n    avg_coef.append(        \n        # average over CV folds\n        np.mean([            \n            # mean absolute value of weights\n            np.mean(np.abs(model['reg'].coef_))\n            for model in results['estimator']\n        ]))\n    \nplot_fitting_graph(DEGREES, rmse_train, rmse_valid,\n                   xlabel='Complexity (degree)', ylabel='Error (RMSE)',\n                   custom_metric=avg_coef, custom_label='avg(|$w_i$|)',\n                   title='Least squares polynomial regression')","metadata":{"ExecuteTime":{"end_time":"2021-12-15T20:32:48.876099Z","start_time":"2021-12-15T20:32:47.820142Z"},"id":"fKJ6oJsgFm7e","outputId":"d01a4b2f-ff0e-423a-9017-aae9af204f49","execution":{"iopub.status.busy":"2022-03-28T07:01:39.110243Z","iopub.execute_input":"2022-03-28T07:01:39.111265Z","iopub.status.idle":"2022-03-28T07:01:40.018136Z","shell.execute_reply.started":"2022-03-28T07:01:39.111219Z","shell.execute_reply":"2022-03-28T07:01:40.017046Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":"### Summary","metadata":{"ExecuteTime":{"end_time":"2020-11-09T15:24:18.245064Z","start_time":"2020-11-09T15:24:18.241548Z"},"id":"e80d-5oiFm7e"}},{"cell_type":"markdown","source":"We observe the following:\n\n1. **Underfitting** (degree < 5): The model is not able to fit the data properly. The fit is bad for both the training and the validation set.\n\n2. **Fit is just right** (degree = 5): The model is able to capture the underlying data distribution. The fit is good for both the training and the validation set.\n\n3. **Overfitting** (degree > 5): The model starts fitting the noise in the dataset. While the fit for the training data gets even better, the fit for the validation set gets worse.\n\n4. As the order of polynomial increases, the linear model coefficients become more likely to take on **large values**.","metadata":{"id":"FIjJPBttFm7e"}},{"cell_type":"markdown","source":"## Part 2: Regularization","metadata":{"ExecuteTime":{"end_time":"2020-11-03T08:47:28.699189Z","start_time":"2020-11-03T08:47:28.695466Z"},"id":"1MT-68ERFm7f"}},{"cell_type":"markdown","source":"There are two major ways to build a machine learning model with the ability to generalize well on unseen data:\n1. Train the simplest model possible for our purpose (according to Occam’s Razor).\n2. Train a complex or more expressive model on the data and perform regularization.\n\nRegularization is a method used to reduce the variance of a machine learning model. In other words, it is used to reduce overfitting. Regularization penalizes a model for being complex. For linear models, it means regularization forces model coefficients to be smaller in magnitude.\n\nLet's pick a polynomial model of degree **15** (which tends to overfit strongly) and try to regularize it using **L1** and **L2** penalties.","metadata":{"id":"vLFkPt07Fm7f"}},{"cell_type":"markdown","source":"### L1 - Lasso regression","metadata":{"id":"Iy0SoHrQFm7g"}},{"cell_type":"code","source":"rmse_train, rmse_valid = [], []\nfor alpha in ALPHAS:    \n    results = cross_validate(make_model(degree=15, penalty='L1', alpha=alpha), \n                             X, y, cv=5,\n                             return_train_score=True,\n                             scoring='neg_root_mean_squared_error')\n    rmse_train.append(-np.mean(results['train_score']))\n    rmse_valid.append(-np.mean(results['test_score']))\n    \nplot_fitting_graph(ALPHAS, rmse_train, rmse_valid,\n                   xlabel='Regularization strength (alpha)', ylabel='Error (RMSE)',\n                   title='Lasso polynomial regression (L1): degree=15')","metadata":{"ExecuteTime":{"end_time":"2021-12-15T20:32:49.851006Z","start_time":"2021-12-15T20:32:48.876099Z"},"id":"P-XUj3bvFm7g","outputId":"5c62b1f7-c4f0-4384-e13e-8b4108962d22","execution":{"iopub.status.busy":"2022-03-28T07:01:40.019502Z","iopub.execute_input":"2022-03-28T07:01:40.019723Z","iopub.status.idle":"2022-03-28T07:01:41.328252Z","shell.execute_reply.started":"2022-03-28T07:01:40.019696Z","shell.execute_reply":"2022-03-28T07:01:41.327366Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":"### L2 - Ridge regression","metadata":{"id":"5BII4Nb5Fm7g"}},{"cell_type":"code","source":"rmse_train, rmse_valid = [], []\nfor alpha in ALPHAS:    \n    results = cross_validate(make_model(degree=15, penalty='L2', alpha=alpha), \n                             X, y, cv=5,\n                             return_train_score=True,\n                             scoring='neg_root_mean_squared_error')\n    rmse_train.append(-np.mean(results['train_score']))\n    rmse_valid.append(-np.mean(results['test_score']))\n    \nplot_fitting_graph(ALPHAS, rmse_train, rmse_valid, \n                   xlabel='Regularization strength (alpha)', ylabel='Error (RMSE)', \n                   title='Ridge polynomial regression (L2): degree=15')","metadata":{"ExecuteTime":{"end_time":"2021-12-15T20:32:51.202745Z","start_time":"2021-12-15T20:32:50.468099Z"},"id":"AH5Iht1TFm7h","outputId":"596c94a4-3ff6-4ffe-9116-cc308700d4b5","execution":{"iopub.status.busy":"2022-03-28T07:01:41.330269Z","iopub.execute_input":"2022-03-28T07:01:41.331450Z","iopub.status.idle":"2022-03-28T07:01:42.366112Z","shell.execute_reply.started":"2022-03-28T07:01:41.331379Z","shell.execute_reply":"2022-03-28T07:01:42.365283Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":"### Summary","metadata":{"ExecuteTime":{"end_time":"2020-11-09T13:47:39.048589Z","start_time":"2020-11-09T13:47:39.044912Z"},"id":"QlA4Poe5Fm7i"}},{"cell_type":"markdown","source":"1. We can control the regularization strength by changing the hyperparameter `alpha`.\n2. Regularized version of the model performs pretty well. Even in case the original original (unregularized) model is heavily overfitting due to excessive complexity.","metadata":{"ExecuteTime":{"end_time":"2020-11-09T13:49:03.993455Z","start_time":"2020-11-09T13:49:03.987472Z"},"id":"LN2iPlfqFm7i"}},{"cell_type":"markdown","source":"## Part 3: Homework assignment (10 points)","metadata":{"ExecuteTime":{"end_time":"2020-11-09T12:14:17.962945Z","start_time":"2020-11-09T12:14:17.959952Z"},"id":"4RV8l5_LFm7i"}},{"cell_type":"markdown","source":"### Excercise 1 - Overfiting and Underfitting (2 points)","metadata":{"ExecuteTime":{"end_time":"2021-12-10T12:27:23.202301Z","start_time":"2021-12-10T12:27:23.185315Z"},"id":"bEy14ckgFm7j"}},{"cell_type":"markdown","source":"Let's work with the diabetes dataset","metadata":{"ExecuteTime":{"end_time":"2021-12-10T07:35:07.485715Z","start_time":"2021-12-10T07:35:07.461799Z"},"id":"uVNhx7sgFm7j"}},{"cell_type":"code","source":"from sklearn.datasets import load_diabetes\ndata = load_diabetes()\nX_diabetes = pd.DataFrame(data['data'], columns=data['feature_names'])\ny_diabetes = pd.DataFrame(data['target'], columns=['target'])\nprint(data['DESCR'])","metadata":{"ExecuteTime":{"end_time":"2021-12-15T20:32:51.45077Z","start_time":"2021-12-15T20:32:51.234725Z"},"id":"pqzlaOg9Fm7j","execution":{"iopub.status.busy":"2022-03-28T07:01:42.367601Z","iopub.execute_input":"2022-03-28T07:01:42.367924Z","iopub.status.idle":"2022-03-28T07:01:42.465537Z","shell.execute_reply.started":"2022-03-28T07:01:42.367882Z","shell.execute_reply":"2022-03-28T07:01:42.464393Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"markdown","source":"Apply model for diabetes dataset with polynomial feature engineering of different degrees. Plot the dependence of train and test error on polynomial degree. Highlight a degree with the best test error. Which degrees cause overfitting/underfitting? Why?","metadata":{"id":"zLIqUoeCFm7k"}},{"cell_type":"code","source":"# Разделим данные на тренировочные и тестовые в соотношении 70 к 30\nprint(f'Размер исходных данных: {X_diabetes.shape, y_diabetes.shape}')\nX_train_diabetes, X_test_diabetes, y_train_diabetes, y_test_diabetes = train_test_split(X_diabetes, y_diabetes, test_size=0.3, random_state=SEED)\nprint(f'\\nРазмер тренировочных данных: {X_train_diabetes.shape,y_train_diabetes.shape}')\nprint(f'Размер тестовых данных:{X_test_diabetes.shape, y_test_diabetes.shape}')\n","metadata":{"id":"4YfFYWA_Fm7l","outputId":"25e9b8e3-2d6c-4ebc-c05d-1f473ac4a42f","execution":{"iopub.status.busy":"2022-03-28T07:03:07.924510Z","iopub.execute_input":"2022-03-28T07:03:07.925108Z","iopub.status.idle":"2022-03-28T07:03:07.932752Z","shell.execute_reply.started":"2022-03-28T07:03:07.925073Z","shell.execute_reply":"2022-03-28T07:03:07.931951Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Перепишем функцию построения модели ( выбор коффициента и метода регуляризации нам понадобится для следующих тасков )\n# так как мощностей не хватает ограничим постронение для 3 фич(age,sex,bmi) с помощью ColumnTransformer\nfrom sklearn.compose import ColumnTransformer\n# и так как данные по условию сакилированы, уберем скалирование\ndef make_modelD(degree,  alpha=0, penalty=None):\n    # linear regression\n    if alpha == 0:\n        regressor = LinearRegression()\n    # lasso regression\",\n    elif penalty == 'L1':\n        regressor = Lasso(alpha=alpha, random_state=SEED, max_iter=100000)\n    # ridge regression\",\n    elif penalty == 'L2':\n        regressor = Ridge(alpha=alpha, random_state=SEED, max_iter=100000)\n    \n    polFiltred = ColumnTransformer([('PolynomialFeatures', PolynomialFeatures(degree, include_bias=(degree == 0)), ['age','sex','bmi'])], remainder='passthrough')\n\n    return Pipeline([\n        ('pol', polFiltred),\n       # ('sca', StandardScaler()),\n        ('reg', regressor)\n    ])","metadata":{"id":"fA-rygEaFm7l","execution":{"iopub.status.busy":"2022-03-28T07:13:26.951427Z","iopub.execute_input":"2022-03-28T07:13:26.952587Z","iopub.status.idle":"2022-03-28T07:13:26.959250Z","shell.execute_reply.started":"2022-03-28T07:13:26.952543Z","shell.execute_reply":"2022-03-28T07:13:26.958464Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"# Немного скорректировала построение графика для наглядности получаемых значений\ndef rmse(y_true, y_pred):\n    return np.sqrt(mean_squared_error(y_true, y_pred))\n\ndef plot_fitting_graphD(x, metric_train, metric_valid, xlabel, ylabel, \n                       custom_metric=None, custom_label='', custom_scale='log', title='Fitting graph'):\n    plt.figure(figsize=(9, 4.5))\n    plt.plot(x, metric_train, label='Training')\n    plt.plot(x, metric_valid, color='C1', label='Test')\n    plt.axvline(x[np.argmin(metric_train)], color='C2', lw=10, alpha=0.2)\n    plt.axvline(x[np.argmin(metric_valid)], color='C1', lw=10, alpha=0.2)\n\n    plt.title(title)\n    plt.xlabel(xlabel)\n    plt.ylabel(ylabel)\n    plt.grid(True)\n    plt.xticks(x, rotation='vertical')\n    plt.legend(loc='center left') \n    plt.yscale(custom_scale)\n    if custom_metric:\n        plt.twinx()\n        plt.yscale(custom_scale)\n        plt.plot(x, custom_metric, alpha=0.2, lw=4, ls='dotted', color='black', label=custom_label) \n        plt.legend(loc='center right')         \n    plt.show()","metadata":{"id":"7PBMMOjlFm7m","execution":{"iopub.status.busy":"2022-03-28T07:13:29.262182Z","iopub.execute_input":"2022-03-28T07:13:29.262971Z","iopub.status.idle":"2022-03-28T07:13:29.272895Z","shell.execute_reply.started":"2022-03-28T07:13:29.262922Z","shell.execute_reply":"2022-03-28T07:13:29.271856Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"code","source":"# Посмотрим влияние ошибки на степень\nDEGREES = np.linspace(1, 10, 0 + 10, dtype=int)\n\nrmse_train, rmse_test, avg_coef = [], [], []\nfor degree in DEGREES:\n    reg = make_modelD(degree).fit(X_train_diabetes, y_train_diabetes)\n    rmse_train.append(rmse(reg.predict(X_train_diabetes), y_train_diabetes))\n    rmse_test.append(rmse(reg.predict(X_test_diabetes), y_test_diabetes))\n    avg_coef.append(np.mean(np.abs(reg['reg'].coef_)))\nplot_fitting_graphD(DEGREES, rmse_train, rmse_test, xlabel='Complexity (degree)', ylabel='Error (RMSE)', custom_metric=avg_coef, custom_label='avg(|$w_i$|)', \n                   title='Least squares polynomial regression')\n\n\n\nprint(f'Оптимальная степень на тренировочных данных: {DEGREES[np.argmin(rmse_train)]}')\nprint(f'Оптимальная степень на тестовых данных: {DEGREES[np.argmin(rmse_test)]}')","metadata":{"id":"1V4kYrq1Fm7m","outputId":"812cd00c-c0c1-4937-897b-9d15976d6528","execution":{"iopub.status.busy":"2022-03-28T07:13:42.159722Z","iopub.execute_input":"2022-03-28T07:13:42.160033Z","iopub.status.idle":"2022-03-28T07:13:43.380581Z","shell.execute_reply.started":"2022-03-28T07:13:42.160003Z","shell.execute_reply":"2022-03-28T07:13:43.379543Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"code","source":"# Попробуем с кроссвалидацией те же степени\nDEGREES = np.linspace(1, 10, 0 + 10, dtype=int)\n\nrmse_train, rmse_test, avg_coef = [], [], []\nfor degree in DEGREES:\n    results = cross_validate(make_modelD(degree),\n                             X_diabetes, y_diabetes, cv=5,\n                             return_train_score=True, return_estimator=True,\n                             scoring='neg_root_mean_squared_error')\n    rmse_train.append(-np.mean(results['train_score']))\n    rmse_test.append(-np.mean(results['test_score']))        \n    avg_coef.append(np.mean([np.mean(np.abs(model['reg'].coef_)) for model in results['estimator']]))\n \nplot_fitting_graphD(DEGREES, rmse_train, rmse_test,\n                   xlabel='Complexity (degree)', ylabel='Error (RMSE)',\n                   custom_metric=avg_coef, custom_label='avg(|$w_i$|)',\n                   title='Least squares polynomial regression')\n\nprint(f'Оптимальная степень на тренировочных данных: {DEGREES[np.argmin(rmse_train)]}')\nprint(f'Оптимальная степень на тестовых данных: {DEGREES[np.argmin(rmse_test)]}')\n","metadata":{"id":"bOr8iw2_Fm7n","outputId":"935070e4-e3ca-4117-f132-200cbf17b54d","execution":{"iopub.status.busy":"2022-03-28T07:24:05.069262Z","iopub.execute_input":"2022-03-28T07:24:05.069566Z","iopub.status.idle":"2022-03-28T07:24:08.118332Z","shell.execute_reply.started":"2022-03-28T07:24:05.069535Z","shell.execute_reply":"2022-03-28T07:24:08.117381Z"},"trusted":true},"execution_count":50,"outputs":[]},{"cell_type":"markdown","source":"Вывод:\n\nМинимальная ошибка на тестовых данных наблюдается на степени (degree = 2). При степени 1 велика вероятность недоученности модели. При степени >2 наблюдается значительный рост ошибки rmse. Значит выбор степени > 2(особенно 4,5,8-10 т.к. наблюдается выбросы весов коэффициентов) может привести к переобученности модели. Особенно  \nГрафик и значения ошибок даже на оптимальных степенях скорее говорит о нехватке исходных данных для обучения, причем кроссавидация не сильно помогла.  ","metadata":{"id":"RwLP_je6Fm7n"}},{"cell_type":"markdown","source":"### Excercise 2 - Magnitude (3 points)","metadata":{"ExecuteTime":{"end_time":"2021-12-10T12:46:46.756169Z","start_time":"2021-12-10T12:44:13.217Z"},"id":"UA4pLbxKFm7o"}},{"cell_type":"markdown","source":"As discussed earlier, regularization methods are expected to constraint the weights (model coefficients). \n\nIs it indeed happening? \n\nPlease do a discovery on your own and find that out empirically (both for **L1** and **L2**). Let's use `degree=15` and `alpha` from `ALPHAS`.","metadata":{"id":"CcysIDYCFm7o"}},{"cell_type":"code","source":"def plot_fitting_graph_coef(x, metric, xlabel, ylabel, \n                       custom_metric=None, custom_label='', custom_scale='log', title='Fitting graph'):\n    plt.figure(figsize=(9, 4.5))\n    plt.plot(x, metric, label='Coefficient')\n    plt.title(title)\n    plt.xlabel(xlabel)\n    plt.ylabel(ylabel)\n    plt.grid(True)\n    plt.xticks(x, rotation='vertical')\n    plt.legend(loc='center left')    \n    plt.yscale(custom_scale)\n    plt.show()","metadata":{"id":"z-rtn1A9Fm7o","execution":{"iopub.status.busy":"2022-03-28T07:24:48.327643Z","iopub.execute_input":"2022-03-28T07:24:48.327981Z","iopub.status.idle":"2022-03-28T07:24:48.334609Z","shell.execute_reply.started":"2022-03-28T07:24:48.327948Z","shell.execute_reply":"2022-03-28T07:24:48.333824Z"},"trusted":true},"execution_count":51,"outputs":[]},{"cell_type":"markdown","source":"#### L1","metadata":{"id":"Azzj2aFDFm7p"}},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"degree_= 15\nALPHAS=np.linspace(0, 0.5, 1 + 40)\n# Чтобы не пересчитывать в 3 задании, сразу сохраним количество обнуленных коеффициентов \nzeroes_amount_L1 = dict()\n\n# В задании посмотреть зависимоть коэффициентов от alpha\navg_coef = []\n# Посмотрим как alpha влияет на ошибку на трнировочных и тестовых данных\nrmse_train, rmse_test= [], []\nfor alpha in ALPHAS:    \n    modelD_L1 = make_modelD(degree=degree_, penalty='L1', alpha=alpha).fit(X_train_diabetes, y_train_diabetes)\n    # средние значения коеффициентов\n    avg_coef.append(np.mean(np.abs(modelD_L1['reg'].coef_)))\n    \n    # ошибки\n    rmse_train.append(rmse(modelD_L1.predict(X_train_diabetes), y_train_diabetes))\n    rmse_test.append(rmse(modelD_L1.predict(X_test_diabetes), y_test_diabetes))\n    \n    # для третьего задания\n    zeroes_amount_L1[alpha] = np.count_nonzero(modelD_L1['reg'].coef_==0)\n    \n                \nplot_fitting_graph_coef(ALPHAS, avg_coef, xlabel='Complexity (alpha)', ylabel='AVG_COEF (avg(|$w_i$|))', title = 'Lasso method'  )\nplot_fitting_graphD(ALPHAS, rmse_train, rmse_test,\n                   xlabel='Complexity (alpha)', ylabel='Error (RMSE)',\n                   custom_metric=avg_coef, custom_label='avg(|$w_i$|)',\n                   title='Lasso influence on prediction')\nprint(f'Оптимальное значение alpha для тренировочных данных: {ALPHAS[np.argmin(rmse_train)]}')\nprint(f'Оптимальное значение alpha для тренировочных данных: {ALPHAS[np.argmin(rmse_test)]}')\n\nprint(f'\\nЗначение ошибки на тестовых данных без Lasso: {rmse_test[0]}')\nprint(f'Значение ошибки на тестовых данных для Lasso: {np.argmin(rmse_test)}')","metadata":{"id":"GyYVbCGyFm7p","outputId":"8ed74d17-bcd6-4938-a2c0-72a08609f085","execution":{"iopub.status.busy":"2022-03-28T07:26:38.219569Z","iopub.execute_input":"2022-03-28T07:26:38.219874Z","iopub.status.idle":"2022-03-28T07:26:43.720871Z","shell.execute_reply.started":"2022-03-28T07:26:38.219842Z","shell.execute_reply":"2022-03-28T07:26:43.719932Z"},"trusted":true},"execution_count":53,"outputs":[]},{"cell_type":"markdown","source":"#### L2","metadata":{"id":"tCw9BYxbFm7q"}},{"cell_type":"code","source":"## Проделаем тоже самое для L2\ndegree_= 15\nALPHAS=np.linspace(0, 0.5, 1 + 40)\n# Для третьего задания \nzeroes_amount_L2 = dict()\n\navg_coef = []\nrmse_train, rmse_test= [], []\n\nfor alpha in ALPHAS: \n    modelD_L2 = make_modelD(degree=degree_, penalty='L2', alpha=alpha).fit(X_train_diabetes, y_train_diabetes)\n    # средние значения коеффициентов\n\n    avg_coef.append(np.mean(np.abs(modelD_L2['reg'].coef_)))\n    \n    # ошибки\n    rmse_train.append(rmse(modelD_L2.predict(X_train_diabetes), y_train_diabetes))\n    rmse_test.append(rmse(modelD_L2.predict(X_test_diabetes), y_test_diabetes))\n     \n    # для третьего задания\n    zeroes_amount_L2[alpha] = np.count_nonzero(modelD_L2['reg'].coef_==0)\n    \n                \nplot_fitting_graph_coef(ALPHAS, avg_coef, xlabel='Complexity (alpha)', ylabel='AVG_COEF (avg(|$w_i$|))',title = 'Ridge method' )\nplot_fitting_graphD(ALPHAS, rmse_train, rmse_test,\n                   xlabel='Complexity (alpha)', ylabel='Error (RMSE)',\n                   custom_metric=avg_coef, custom_label='avg(|$w_i$|)',\n                   title='Least squares polynomial regression')\nprint(f'Оптимальное значение alpha для тренировочных данных: {ALPHAS[np.argmin(rmse_train)]}')\nprint(f'Оптимальное значение alpha для тренировочных данных: {ALPHAS[np.argmin(rmse_test)]}')\n\nprint(f'\\nЗначение ошибки на тестовых данных без Ridge: {rmse_test[0]}')\nprint(f'Значение ошибки на тестовых данных для Ridge: {np.argmin(rmse_test)}')\n","metadata":{"id":"vqYOXRiFFm7q","outputId":"f26d4b62-cfc2-498d-b4c8-533795ee8675","execution":{"iopub.status.busy":"2022-03-28T07:27:25.641707Z","iopub.execute_input":"2022-03-28T07:27:25.641970Z","iopub.status.idle":"2022-03-28T07:27:31.095223Z","shell.execute_reply.started":"2022-03-28T07:27:25.641943Z","shell.execute_reply":"2022-03-28T07:27:31.094062Z"},"trusted":true},"execution_count":54,"outputs":[]},{"cell_type":"markdown","source":"#### Summary","metadata":{"id":"F0XMdnB-Fm7r"}},{"cell_type":"markdown","source":"Как и ожидалось, с увеличением alpha уменьшаются веса коэффициентов модели. \nМетоды регуляризации Lasso и Ridge одинаково хорошо себя проявили на тестируемых датасетах","metadata":{}},{"cell_type":"markdown","source":"### Excercise 3 - Sparsity (3 points)","metadata":{"ExecuteTime":{"end_time":"2021-12-10T12:46:46.756169Z","start_time":"2021-12-10T12:44:13.217Z"},"id":"9PNU0CEGFm7t"}},{"cell_type":"markdown","source":"Lasso can also be used for **feature selection** since L1 is [more likely to produce zero coefficients](https://explained.ai/regularization/).\n\nIs it indeed happening? \n\nPlease do a discovery on your own and find that out empirically (both for **L1** and **L2**). Let's use `degree=15` and `alpha` from `ALPHAS`.","metadata":{"id":"aE279b-VFm7t"}},{"cell_type":"markdown","source":"#### L1","metadata":{"id":"4vcWycZuFm7u"}},{"cell_type":"code","source":"## построим график зависимостей кол-ва обнуленных коэффициентов от alpha\nfig, ax = plt.subplots(figsize = (9,4.5))\n\ndt = pd.DataFrame(zeroes_amount_L1, index = zeroes_amount_L1.keys())\n\nplt.plot( zeroes_amount_L1.keys(), zeroes_amount_L1.values())\nplt.show()\n\n","metadata":{"id":"WWursO1jFm7u","execution":{"iopub.status.busy":"2022-03-28T07:32:47.870192Z","iopub.execute_input":"2022-03-28T07:32:47.870508Z","iopub.status.idle":"2022-03-28T07:32:48.056890Z","shell.execute_reply.started":"2022-03-28T07:32:47.870479Z","shell.execute_reply":"2022-03-28T07:32:48.056255Z"},"trusted":true},"execution_count":64,"outputs":[]},{"cell_type":"markdown","source":"#### L2","metadata":{"id":"Q5tok0faFm7u"}},{"cell_type":"code","source":"## для метода Ridge нулевых не получилось\nres = all(x == 0 for x in zeroes_amount_L2.values())\nprint(f'Нулевые коеффициенты отсутствуют: {res}')","metadata":{"id":"Uvm1XlQGFm7v","execution":{"iopub.status.busy":"2022-03-28T07:28:03.937826Z","iopub.execute_input":"2022-03-28T07:28:03.938088Z","iopub.status.idle":"2022-03-28T07:28:03.943407Z","shell.execute_reply.started":"2022-03-28T07:28:03.938061Z","shell.execute_reply":"2022-03-28T07:28:03.942765Z"},"trusted":true},"execution_count":56,"outputs":[]},{"cell_type":"markdown","source":"#### Summary","metadata":{"id":"HoIs4QMZFm7v"}},{"cell_type":"markdown","source":"Как и ожидалось, метод Lasso привел к обнулению коэффициентов,причем с увеличением alpha больше коэффициентов обнуляется.\nВ методе Ridge вес коэффициентов уменьшается с увеличением alpha, однако обнуления не происходит.","metadata":{}},{"cell_type":"markdown","source":"### Excercise 4 - Scaling (2 points)","metadata":{"id":"RKcMggDQFm7w"}},{"cell_type":"markdown","source":"As a general rule, it is recommended to scale input features before fitting a regularized model so that the features/inputs take values in similar ranges. One common way of doing so is to standardize the inputs and that is exactly what our pipeline  second step (`StandardScaler`) is responsible for. \n\nIs scaling important? What are the underlying reasons?\n\nPlease do a discovery on your own and find that out empirically (both for **L1** and **L2**) on the dataset below. Check coefficients.","metadata":{"id":"odvQdZTUFm7w"}},{"cell_type":"code","source":"def target_function_hw(x):\n    return 2 * x\n\ndef generate_samples_hw():\n    np.random.seed(SEED)\n    x = np.random.uniform(*RANGE, size=N_SAMPLES)\n    \n    np.random.seed(SEED+1)\n    x_noise = np.random.uniform(*[x * 100 for x in RANGE], size=N_SAMPLES)\n    x_noise2 = np.random.normal(100, 50, size=N_SAMPLES)\n    \n    y = target_function_hw(x) + np.random.normal(scale=4, size=N_SAMPLES)\n    \n    return np.concatenate([x.reshape(-1, 1) / 100, \n                           x_noise.reshape(-1, 1),\n                           x_noise2.reshape(-1, 1)], axis=1), y\n\nX_hw, y_hw = generate_samples_hw()\n\nfor i in range(X_hw.shape[1]):\n    print(f'Min of feature {i}: {min(X_hw[:, i]):.2f}, max: {max(X_hw[:, i]):.2f}')","metadata":{"id":"7JJlRDbDFm7x","outputId":"8227cc29-1bf5-4ab5-cd13-683e0ceabb31","execution":{"iopub.status.busy":"2022-03-28T07:33:43.440109Z","iopub.execute_input":"2022-03-28T07:33:43.440387Z","iopub.status.idle":"2022-03-28T07:33:43.450149Z","shell.execute_reply.started":"2022-03-28T07:33:43.440357Z","shell.execute_reply":"2022-03-28T07:33:43.449441Z"},"trusted":true},"execution_count":65,"outputs":[]},{"cell_type":"code","source":"# Построим модель без скалирования\ndef make_model_Noscalling(degree,  alpha=0, penalty=None):\n    # linear regression\n    if alpha == 0:\n        regressor = LinearRegression()\n    # lasso regression\",\n    elif penalty == 'L1':\n        regressor = Lasso(alpha=alpha, random_state=SEED, max_iter=100000)\n    # ridge regression\",\n    elif penalty == 'L2':\n        regressor = Ridge(alpha=alpha, random_state=SEED, max_iter=100000) \n    \n    \n    return Pipeline([\n        ('pol', PolynomialFeatures(degree, include_bias=(degree == 0))),\n        \n        ('reg', regressor)\n    ])","metadata":{"execution":{"iopub.status.busy":"2022-03-28T07:34:49.513226Z","iopub.execute_input":"2022-03-28T07:34:49.513537Z","iopub.status.idle":"2022-03-28T07:34:49.519588Z","shell.execute_reply.started":"2022-03-28T07:34:49.513506Z","shell.execute_reply":"2022-03-28T07:34:49.518696Z"},"trusted":true},"execution_count":70,"outputs":[]},{"cell_type":"markdown","source":"#### L1","metadata":{"id":"Y89bxdD1Fm7x"}},{"cell_type":"code","source":"## your code\nDEGREES = np.linspace(1, 10, 0 + 10, dtype=int)\nrmse_train, rmse_valid= [], []\n# Найдем оптимальную степень\nfor degree in DEGREES:\n    results = cross_validate(make_model(degree),\n                             X_hw, y_hw, cv=5,\n                             return_train_score=True, return_estimator=True,\n                             scoring='neg_root_mean_squared_error') \n    rmse_train.append(-np.mean(results['train_score']))\n    rmse_valid.append(-np.mean(results['test_score']))\n\nprint(f'Оптимальная степень на тестовых данных: {DEGREES[np.argmin(rmse_valid)]}') \ndegree_= DEGREES[np.argmin(rmse_valid)]\n","metadata":{"id":"8DQpRV0ZFm7y","execution":{"iopub.status.busy":"2022-03-28T07:34:25.498617Z","iopub.execute_input":"2022-03-28T07:34:25.498899Z","iopub.status.idle":"2022-03-28T07:34:25.790343Z","shell.execute_reply.started":"2022-03-28T07:34:25.498870Z","shell.execute_reply":"2022-03-28T07:34:25.788185Z"},"trusted":true},"execution_count":67,"outputs":[]},{"cell_type":"code","source":"# Найдем оптимальную alpha\nALPHAS=np.linspace(0, 0.5, 1 + 40)\nrmse_valid = []\nfor alpha in ALPHAS:\n    results = cross_validate(make_model(degree=degree_, penalty='L1', alpha=alpha),\n                             X_hw, y_hw, cv=5,\n                             return_train_score=True, return_estimator=True,\n                             scoring='neg_root_mean_squared_error') \n    rmse_valid.append(-np.mean(results['test_score']))\n\nprint(f'Оптимальная alpha на тестовых данных: {ALPHAS[np.argmin(rmse_valid)]}') \nalpha_= ALPHAS[np.argmin(rmse_valid)]\n","metadata":{"execution":{"iopub.status.busy":"2022-03-28T07:33:46.288379Z","iopub.execute_input":"2022-03-28T07:33:46.288753Z","iopub.status.idle":"2022-03-28T07:34:12.938787Z","shell.execute_reply.started":"2022-03-28T07:33:46.288724Z","shell.execute_reply":"2022-03-28T07:34:12.937868Z"},"trusted":true},"execution_count":66,"outputs":[]},{"cell_type":"code","source":"X_hw_train, X_hw_test, y_hw_train, y_hw_test = train_test_split(X_hw, y_hw, test_size=0.3, random_state=SEED)\n\nmodel = make_model(degree=degree_,penalty='L1', alpha=alpha_).fit(X_hw_train, y_hw_train)\ny_pred_train = model.predict(X_hw_train)\ny_pred_test = model.predict(X_hw_test)\nrmse_train =rmse(y_pred_train,y_hw_train)\nrmse_test = rmse(y_pred_test,y_hw_test)\n\nprint(f'Ошибка RMSE для скалированных данных с регуляризацией Lasso: {rmse_test}')","metadata":{"execution":{"iopub.status.busy":"2022-03-28T07:34:31.547884Z","iopub.execute_input":"2022-03-28T07:34:31.548174Z","iopub.status.idle":"2022-03-28T07:34:31.559567Z","shell.execute_reply.started":"2022-03-28T07:34:31.548143Z","shell.execute_reply":"2022-03-28T07:34:31.558524Z"},"trusted":true},"execution_count":68,"outputs":[]},{"cell_type":"code","source":"model = make_model_Noscalling(degree=degree_,penalty='L1', alpha=alpha_).fit(X_hw_train, y_hw_train)\ny_pred_train = model.predict(X_hw_train)\ny_pred_test = model.predict(X_hw_test)\nrmse_train =rmse(y_pred_train,y_hw_train)\nrmse_test = rmse(y_pred_test,y_hw_test)\n\nprint(f'Ошибка RMSE для нескалированных данных с регуляризацией Lasso: {rmse_test}')","metadata":{"execution":{"iopub.status.busy":"2022-03-28T07:35:22.145134Z","iopub.execute_input":"2022-03-28T07:35:22.145445Z","iopub.status.idle":"2022-03-28T07:35:22.155768Z","shell.execute_reply.started":"2022-03-28T07:35:22.145402Z","shell.execute_reply":"2022-03-28T07:35:22.155046Z"},"trusted":true},"execution_count":71,"outputs":[]},{"cell_type":"markdown","source":"#### L2","metadata":{"id":"77Uv7hJ5Fm7y"}},{"cell_type":"code","source":"# Найдем оптимальную alpha\nALPHAS=np.linspace(0, 0.5, 1 + 40)\nrmse_valid = []\nfor alpha in ALPHAS:\n    results = cross_validate(make_model(degree=degree_, penalty='L2', alpha=alpha),\n                             X_hw, y_hw, cv=5,\n                             return_train_score=True, return_estimator=True,\n                             scoring='neg_root_mean_squared_error') \n    rmse_valid.append(-np.mean(results['test_score']))\n\nprint(f'Оптимальная alpha на тестовых данных: {ALPHAS[np.argmin(rmse_valid)]}') \nalpha_= ALPHAS[np.argmin(rmse_valid)]","metadata":{"id":"9Lt-zPk5Fm7z","execution":{"iopub.status.busy":"2022-03-28T07:35:28.583983Z","iopub.execute_input":"2022-03-28T07:35:28.584716Z","iopub.status.idle":"2022-03-28T07:35:29.222262Z","shell.execute_reply.started":"2022-03-28T07:35:28.584675Z","shell.execute_reply":"2022-03-28T07:35:29.220980Z"},"trusted":true},"execution_count":72,"outputs":[]},{"cell_type":"code","source":"model = make_model(degree=degree_,penalty='L2', alpha=alpha_).fit(X_hw_train, y_hw_train)\ny_pred_train = model.predict(X_hw_train)\ny_pred_test = model.predict(X_hw_test)\nrmse_train =rmse(y_pred_train,y_hw_train)\nrmse_test = rmse(y_pred_test,y_hw_test)\n\nprint(f'Ошибка RMSE для скалированных данных с регуляризацией Ridge: {rmse_test}')\n\nmodel = make_model_Noscalling(degree=degree_,penalty='L2', alpha=alpha_).fit(X_hw_train, y_hw_train)\ny_pred_train = model.predict(X_hw_train)\ny_pred_test = model.predict(X_hw_test)\nrmse_train =rmse(y_pred_train,y_hw_train)\nrmse_test = rmse(y_pred_test,y_hw_test)\n\nprint(f'Ошибка RMSE для нескалированных данных с регуляризацией Ridge: {rmse_test}')\n","metadata":{"execution":{"iopub.status.busy":"2022-03-28T07:35:40.841442Z","iopub.execute_input":"2022-03-28T07:35:40.842048Z","iopub.status.idle":"2022-03-28T07:35:40.854842Z","shell.execute_reply.started":"2022-03-28T07:35:40.841995Z","shell.execute_reply":"2022-03-28T07:35:40.854138Z"},"trusted":true},"execution_count":73,"outputs":[]},{"cell_type":"markdown","source":"В проведенном экперименте мы убедились, что сканирование данных приводит уменьшению ошибки. А также, что для данных данных предпочтительней строить модель с применением метода Lasso.","metadata":{}}]}