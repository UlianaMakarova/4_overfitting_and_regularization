{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Overfitting andÂ Regularization","metadata":{"id":"57oHIczrFm68"}},{"cell_type":"markdown","source":"## Imports","metadata":{"id":"ltJHzh--Fm6_"}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt","metadata":{"ExecuteTime":{"end_time":"2022-02-16T07:39:25.890581Z","start_time":"2022-02-16T07:39:23.835921Z"},"id":"zh_Uf84JFm7A","execution":{"iopub.status.busy":"2022-03-28T07:01:34.637468Z","iopub.execute_input":"2022-03-28T07:01:34.637839Z","iopub.status.idle":"2022-03-28T07:01:34.782118Z","shell.execute_reply.started":"2022-03-28T07:01:34.637793Z","shell.execute_reply":"2022-03-28T07:01:34.781180Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"from sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import train_test_split, cross_validate\nfrom sklearn.preprocessing import PolynomialFeatures, StandardScaler\nfrom sklearn.linear_model import LinearRegression, Lasso, Ridge\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn import set_config","metadata":{"ExecuteTime":{"end_time":"2022-02-16T07:39:26.108031Z","start_time":"2022-02-16T07:39:25.925893Z"},"id":"38FANhVpFm7C","execution":{"iopub.status.busy":"2022-03-28T07:01:34.784465Z","iopub.execute_input":"2022-03-28T07:01:34.784961Z","iopub.status.idle":"2022-03-28T07:01:34.926226Z","shell.execute_reply.started":"2022-03-28T07:01:34.784927Z","shell.execute_reply":"2022-03-28T07:01:34.925270Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"set_config(display='diagram')","metadata":{"ExecuteTime":{"end_time":"2022-02-16T07:39:26.155183Z","start_time":"2022-02-16T07:39:26.141249Z"},"id":"Dgj99jtfFm7E","execution":{"iopub.status.busy":"2022-03-28T07:01:34.928630Z","iopub.execute_input":"2022-03-28T07:01:34.928854Z","iopub.status.idle":"2022-03-28T07:01:34.933972Z","shell.execute_reply.started":"2022-03-28T07:01:34.928827Z","shell.execute_reply":"2022-03-28T07:01:34.933010Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"## Settings","metadata":{"id":"X6o8fzTtFm7F"}},{"cell_type":"code","source":"SEED = 42\nRANGE = (-5, 5)\nN_SAMPLES = 50\nDEGREES = np.linspace(0, 15, 1 + 15, dtype=int)\nALPHAS = np.linspace(0, 0.5, 1 + 40)","metadata":{"ExecuteTime":{"end_time":"2022-02-16T07:39:26.201661Z","start_time":"2022-02-16T07:39:26.189181Z"},"id":"6D17-LAMFm7G","execution":{"iopub.status.busy":"2022-03-28T07:01:34.954215Z","iopub.execute_input":"2022-03-28T07:01:34.955170Z","iopub.status.idle":"2022-03-28T07:01:34.961689Z","shell.execute_reply.started":"2022-03-28T07:01:34.955024Z","shell.execute_reply":"2022-03-28T07:01:34.961008Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"## Part 1: Underfitting vs. overfitting","metadata":{"id":"NYbabdyBFm7H"}},{"cell_type":"markdown","source":"### Generate samples","metadata":{"id":"NoBHaja_Fm7I"}},{"cell_type":"markdown","source":"Let's pick a target function $ f(x) = 2\\cdot x + 10\\cdot sin(x) $ and generate some noisy samples to learn from.","metadata":{"ExecuteTime":{"end_time":"2020-11-09T09:04:09.904994Z","start_time":"2020-11-09T09:04:09.896444Z"},"id":"pHVooi1MFm7J"}},{"cell_type":"code","source":"def target_function(x):\n    return 2 * x + 10 * np.sin(x)\n\ndef generate_samples():\n    \"\"\"Generate noisy samples.\"\"\"\n    np.random.seed(SEED)\n    x = np.random.uniform(*RANGE, size=N_SAMPLES)\n    y = target_function(x) + np.random.normal(scale=4, size=N_SAMPLES)\n    return x.reshape(-1, 1), y\n\nX, y = generate_samples()","metadata":{"ExecuteTime":{"end_time":"2022-02-16T07:39:26.433239Z","start_time":"2022-02-16T07:39:26.412501Z"},"id":"opI1jaZoFm7K","execution":{"iopub.status.busy":"2022-03-28T07:01:35.044296Z","iopub.execute_input":"2022-03-28T07:01:35.044721Z","iopub.status.idle":"2022-03-28T07:01:35.052682Z","shell.execute_reply.started":"2022-03-28T07:01:35.044691Z","shell.execute_reply":"2022-03-28T07:01:35.052017Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"### Plot samples","metadata":{"id":"sS6SKtVtFm7M"}},{"cell_type":"code","source":"def plot_scatter(x, y, title=None, label='Noisy samples'):\n    plt.scatter(x, y, label=label)\n    plt.xlabel('x')\n    plt.ylabel('y')\n    plt.grid(True)\n    plt.title(title)\n    plt.legend(loc='lower right')\n\nplot_scatter(X, y)\n","metadata":{"ExecuteTime":{"end_time":"2022-02-16T07:39:27.822431Z","start_time":"2022-02-16T07:39:27.696426Z"},"id":"5YGLVTBCFm7N","outputId":"d61276f7-a44b-45d3-bbf9-8d31b16635a6","execution":{"iopub.status.busy":"2022-03-28T07:01:35.121676Z","iopub.execute_input":"2022-03-28T07:01:35.122065Z","iopub.status.idle":"2022-03-28T07:01:35.372063Z","shell.execute_reply.started":"2022-03-28T07:01:35.122034Z","shell.execute_reply":"2022-03-28T07:01:35.371237Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"### Split","metadata":{"id":"aduHf6u4Fm7O"}},{"cell_type":"code","source":"X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.3, random_state=SEED)\n\nplot_scatter(X_train, y_train, label='Training set')\nplot_scatter(X_valid, y_valid, label='Validation set')","metadata":{"ExecuteTime":{"end_time":"2022-02-16T07:39:29.210072Z","start_time":"2022-02-16T07:39:29.093175Z"},"id":"wErymr7TFm7P","outputId":"aac84695-2f4e-4953-aa2c-cc564fb49023","execution":{"iopub.status.busy":"2022-03-28T07:01:35.373673Z","iopub.execute_input":"2022-03-28T07:01:35.373922Z","iopub.status.idle":"2022-03-28T07:01:35.597715Z","shell.execute_reply.started":"2022-03-28T07:01:35.373891Z","shell.execute_reply":"2022-03-28T07:01:35.596719Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"y_train","metadata":{"ExecuteTime":{"end_time":"2022-02-16T07:39:33.496625Z","start_time":"2022-02-16T07:39:33.479851Z"},"id":"BacqE9YRFm7P","outputId":"70e69aa1-e3f9-41d0-c11a-fab5bcedf528","execution":{"iopub.status.busy":"2022-03-28T07:01:35.598976Z","iopub.execute_input":"2022-03-28T07:01:35.599275Z","iopub.status.idle":"2022-03-28T07:01:35.606582Z","shell.execute_reply.started":"2022-03-28T07:01:35.599235Z","shell.execute_reply":"2022-03-28T07:01:35.605635Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"### Model","metadata":{"id":"Yo-PUOOnFm7Q"}},{"cell_type":"markdown","source":"Let's try to approximate our target function $ f(x) = 2\\cdot x + 10\\cdot sin(x) $ with polynomials of different degree. \n\nA polynomial of degree $n$ has the form:\n$ h(x) = w_0 + w_1\\cdot x + w_2\\cdot x^2 +\\ldots + w_n\\cdot x^n $.\n\n$x^i$ values could easily be generated by `PolynomialFeatures`, while $w_i$ are the unknown paramaters to be estimated using `LinearRegression`.","metadata":{"id":"zXaoIFisFm7S"}},{"cell_type":"code","source":"PolynomialFeatures(degree=4, include_bias=False).fit_transform(X=[\n    [1],\n    [3],\n    [4],\n])\n","metadata":{"ExecuteTime":{"end_time":"2022-02-16T07:39:34.647226Z","start_time":"2022-02-16T07:39:34.63954Z"},"id":"ZYNgwxnvFm7S","outputId":"b4e115d2-801d-45ed-e6ee-be5c33ab9b47","execution":{"iopub.status.busy":"2022-03-28T07:01:35.608682Z","iopub.execute_input":"2022-03-28T07:01:35.608990Z","iopub.status.idle":"2022-03-28T07:01:35.622163Z","shell.execute_reply.started":"2022-03-28T07:01:35.608948Z","shell.execute_reply":"2022-03-28T07:01:35.621387Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"def make_model(degree, alpha=0, penalty=None):\n    # linear regression\n    if alpha == 0:\n        regressor = LinearRegression()\n    # lasso regression\",\n    elif penalty == 'L1':\n        regressor = Lasso(alpha=alpha, random_state=SEED, max_iter=50000)\n    # ridge regression\",\n    elif penalty == 'L2':\n        regressor = Ridge(alpha=alpha, random_state=SEED, max_iter=50000) \n    \n    \n    return Pipeline([\n        ('pol', PolynomialFeatures(degree, include_bias=(degree == 0))),\n        ('sca', StandardScaler()),\n        ('reg', regressor)\n    ])\n\ndisplay(make_model(2))\ndisplay(make_model(2, penalty='L1', alpha=0.1))\ndisplay(make_model(2, penalty='L2', alpha=0.1))","metadata":{"ExecuteTime":{"end_time":"2022-02-16T07:40:49.993908Z","start_time":"2022-02-16T07:40:49.95934Z"},"id":"iDmw0hkJFm7T","outputId":"88f19040-8f35-487e-d198-56216b56c510","execution":{"iopub.status.busy":"2022-03-28T07:01:35.623072Z","iopub.execute_input":"2022-03-28T07:01:35.623286Z","iopub.status.idle":"2022-03-28T07:01:35.657822Z","shell.execute_reply.started":"2022-03-28T07:01:35.623259Z","shell.execute_reply":"2022-03-28T07:01:35.656857Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"### Fit","metadata":{"id":"qBWLs5_WFm7T"}},{"cell_type":"markdown","source":"Let's fit a model and plot the hypothesis it learns:","metadata":{"id":"2e9sYayaFm7U"}},{"cell_type":"code","source":"def plot_fit(model):\n    degree = model['pol'].degree\n    X_range = np.linspace(*RANGE, 1000).reshape(-1, 1)\n    y_pred = model.predict(X_range)\n    plot_scatter(X_train, y_train, label='Training sample')\n    plot_scatter(X_valid, y_valid, label='Validation sample')\n    plt.plot(X_range, target_function(X_range), c='green', alpha=0.2, lw=5, label='Target function')\n    plt.plot(X_range, y_pred, c='red', label='Hypothesis')\n    plt.ylim((min(y) - 3, max(y) + 3))\n    plt.legend(loc='best')    \n    plt.title(f'Polynomial approximation: degree={degree}')\n    plt.show()\n\nplot_fit(make_model(degree=2).fit(X_train, y_train))","metadata":{"ExecuteTime":{"end_time":"2022-02-16T07:40:51.426785Z","start_time":"2022-02-16T07:40:51.20983Z"},"id":"jC8VPvg_Fm7U","outputId":"f6cb91a4-9f00-4d7e-829e-1cfab85bc4a6","execution":{"iopub.status.busy":"2022-03-28T07:01:35.659228Z","iopub.execute_input":"2022-03-28T07:01:35.659780Z","iopub.status.idle":"2022-03-28T07:01:35.912925Z","shell.execute_reply.started":"2022-03-28T07:01:35.659733Z","shell.execute_reply":"2022-03-28T07:01:35.912062Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"### From underfitting to overfitting","metadata":{"id":"SCFYtnxwFm7V"}},{"cell_type":"markdown","source":"We can investigate the shape of the fitted curve for different values of `degree`:","metadata":{"ExecuteTime":{"end_time":"2020-11-09T11:15:24.323458Z","start_time":"2020-11-09T11:15:24.318089Z"},"id":"i2jduhA1Fm7V"}},{"cell_type":"code","source":"for degree in [0, 1, 2, 3, 4, 5, 10, 15, 20]:\n    plot_fit(make_model(degree).fit(X_train, y_train))","metadata":{"ExecuteTime":{"end_time":"2022-02-16T07:40:54.236522Z","start_time":"2022-02-16T07:40:52.748262Z"},"id":"hs_Vg94NFm7W","outputId":"f97005e7-5a36-4042-98de-e8a4a7415632","execution":{"iopub.status.busy":"2022-03-28T07:01:35.915938Z","iopub.execute_input":"2022-03-28T07:01:35.916287Z","iopub.status.idle":"2022-03-28T07:01:38.316955Z","shell.execute_reply.started":"2022-03-28T07:01:35.916213Z","shell.execute_reply":"2022-03-28T07:01:38.314927Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"### Fitting graph","metadata":{"id":"wy8YP1UaFm7W"}},{"cell_type":"markdown","source":"In the next step we calculate the training and the validation error for each `degree` and plot them in a single graph. The resulting graph is called the fitting graph.","metadata":{"id":"nhCr-Hc5Fm7W"}},{"cell_type":"code","source":"def rmse(y_true, y_pred):\n    return np.sqrt(mean_squared_error(y_true, y_pred))\n\ndef plot_fitting_graph(x, metric_train, metric_valid, xlabel, ylabel, \n                       custom_metric=None, custom_label='', custom_scale='log', title='Fitting graph'):\n    plt.figure(figsize=(9, 4.5))\n    plt.plot(x, metric_train, label='Training')\n    plt.plot(x, metric_valid, color='C1', label='Validation')\n    plt.axvline(x[np.argmin(metric_valid)], color='C1', lw=10, alpha=0.2)\n    plt.title(title)\n    plt.xlabel(xlabel)\n    plt.ylabel(ylabel)\n    plt.grid(True)\n    plt.xticks(x, rotation='vertical')\n    plt.legend(loc='center left')        \n    if custom_metric:\n        plt.twinx()\n        plt.yscale(custom_scale)\n        plt.plot(x, custom_metric, alpha=0.2, lw=4, ls='dotted', color='black', label=custom_label) \n        plt.legend(loc='center right')         \n    plt.show()\n    \nrmse_train, rmse_valid = [], []\nfor degree in DEGREES:\n    reg = make_model(degree).fit(X_train, y_train)\n    rmse_train.append(rmse(reg.predict(X_train), y_train))\n    rmse_valid.append(rmse(reg.predict(X_valid), y_valid))\n    \nplot_fitting_graph(DEGREES, rmse_train, rmse_valid, xlabel='Complexity (degree)', ylabel='Error (RMSE)', \n                   title='Least squares polynomial regression')","metadata":{"ExecuteTime":{"end_time":"2021-12-15T20:32:47.308471Z","start_time":"2021-12-15T20:32:47.036535Z"},"id":"DdXzPB1yFm7W","outputId":"7b2291d1-ed7c-4c67-d75f-9db033ce8583","execution":{"iopub.status.busy":"2022-03-28T07:01:38.318162Z","iopub.execute_input":"2022-03-28T07:01:38.318428Z","iopub.status.idle":"2022-03-28T07:01:38.609685Z","shell.execute_reply.started":"2022-03-28T07:01:38.318395Z","shell.execute_reply":"2022-03-28T07:01:38.608795Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"### Sweet spot","metadata":{"id":"n3tK-5MDFm7X"}},{"cell_type":"markdown","source":"What is the optimal `degree` to go with?","metadata":{"id":"GrfnVaQeFm7X"}},{"cell_type":"code","source":"DEGREES[np.argmin(rmse_valid)]","metadata":{"ExecuteTime":{"end_time":"2021-12-15T20:32:47.324492Z","start_time":"2021-12-15T20:32:47.308471Z"},"id":"tb_JDJhnFm7X","outputId":"011de964-9ca1-47af-84db-b455bfdde37d","execution":{"iopub.status.busy":"2022-03-28T07:01:38.611265Z","iopub.execute_input":"2022-03-28T07:01:38.611856Z","iopub.status.idle":"2022-03-28T07:01:38.618869Z","shell.execute_reply.started":"2022-03-28T07:01:38.611810Z","shell.execute_reply":"2022-03-28T07:01:38.617899Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"### Cross-validation","metadata":{"id":"MB-nw8qeFm7Y"}},{"cell_type":"markdown","source":"Ideally, we would choose the the model parameters such that we have the best model performance. However, we want to make sure that we really have the best validation performance. When we do `train_test_split` we randomly split the data into two parts. What could happen is that we got lucky and split the data such that it favours the validation error. This is especially dangerous if we are dealing with small datasets. One way to check if that's the case is to run the experiment several times for different, random splits. However, there is an even more systematic way of doing this: [cross-validation](https://scikit-learn.org/stable/modules/cross_validation.html).","metadata":{"id":"_Lhn-3VNFm7Y"}},{"cell_type":"markdown","source":"<img src=\"https://scikit-learn.org/stable/_images/grid_search_cross_validation.png\" width=50% />","metadata":{"id":"vWdUUAK9Fm7Z"}},{"cell_type":"code","source":"rmse_train, rmse_valid = [], []\nfor degree in DEGREES:\n    results = cross_validate(make_model(degree), \n                             X, y, cv=5,\n                             return_train_score=True,\n                             scoring='neg_root_mean_squared_error')\n    rmse_train.append(-np.mean(results['train_score']))\n    rmse_valid.append(-np.mean(results['test_score']))\n    \nplot_fitting_graph(DEGREES, rmse_train, rmse_valid, xlabel='Complexity (degree)', ylabel='Error (RMSE)',\n                   title='Least squares polynomial regression')","metadata":{"ExecuteTime":{"end_time":"2021-12-15T20:32:47.788153Z","start_time":"2021-12-15T20:32:47.324492Z"},"id":"aMVRzMcUFm7a","outputId":"1b03bbbe-5f87-44ec-86f3-7df85cda3802","execution":{"iopub.status.busy":"2022-03-28T07:01:38.621503Z","iopub.execute_input":"2022-03-28T07:01:38.621824Z","iopub.status.idle":"2022-03-28T07:01:39.094685Z","shell.execute_reply.started":"2022-03-28T07:01:38.621781Z","shell.execute_reply":"2022-03-28T07:01:39.094021Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":"### Model coefficients","metadata":{"id":"pixxFf3FFm7b"}},{"cell_type":"markdown","source":"Let's inspect our regression model coefficients:","metadata":{"ExecuteTime":{"end_time":"2020-11-05T16:26:33.200639Z","start_time":"2020-11-05T16:26:33.197656Z"},"id":"lnIce5Q_Fm7c"}},{"cell_type":"code","source":"(make_model(degree=1).fit(X_train, y_train)['reg'].coef_,\n make_model(degree=2).fit(X_train, y_train)['reg'].coef_,\n make_model(degree=5).fit(X_train, y_train)['reg'].coef_,\n make_model(degree=10).fit(X_train, y_train)['reg'].coef_)","metadata":{"ExecuteTime":{"end_time":"2021-12-15T20:32:47.820142Z","start_time":"2021-12-15T20:32:47.788153Z"},"id":"JDJWBtJiFm7c","outputId":"0ea211eb-8149-4290-c957-1b883af41e72","execution":{"iopub.status.busy":"2022-03-28T07:01:39.095583Z","iopub.execute_input":"2022-03-28T07:01:39.096087Z","iopub.status.idle":"2022-03-28T07:01:39.108892Z","shell.execute_reply.started":"2022-03-28T07:01:39.096054Z","shell.execute_reply":"2022-03-28T07:01:39.108275Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":"Hmm... it looks like high degree polynomials are coming with much bigger regression coefficients. \n\nWe are going to plot the mean absolute value of $w_i$ as a function of degree to reveal the relationship:","metadata":{"id":"7YcTRbhFFm7d"}},{"cell_type":"code","source":"rmse_train, rmse_valid, avg_coef = [], [], []\nfor degree in DEGREES:\n    results = cross_validate(make_model(degree),\n                             X, y, cv=5,\n                             return_train_score=True, return_estimator=True,\n                             scoring='neg_root_mean_squared_error')\n    rmse_train.append(-np.mean(results['train_score']))\n    rmse_valid.append(-np.mean(results['test_score']))        \n    avg_coef.append(        \n        # average over CV folds\n        np.mean([            \n            # mean absolute value of weights\n            np.mean(np.abs(model['reg'].coef_))\n            for model in results['estimator']\n        ]))\n    \nplot_fitting_graph(DEGREES, rmse_train, rmse_valid,\n                   xlabel='Complexity (degree)', ylabel='Error (RMSE)',\n                   custom_metric=avg_coef, custom_label='avg(|$w_i$|)',\n                   title='Least squares polynomial regression')","metadata":{"ExecuteTime":{"end_time":"2021-12-15T20:32:48.876099Z","start_time":"2021-12-15T20:32:47.820142Z"},"id":"fKJ6oJsgFm7e","outputId":"d01a4b2f-ff0e-423a-9017-aae9af204f49","execution":{"iopub.status.busy":"2022-03-28T07:01:39.110243Z","iopub.execute_input":"2022-03-28T07:01:39.111265Z","iopub.status.idle":"2022-03-28T07:01:40.018136Z","shell.execute_reply.started":"2022-03-28T07:01:39.111219Z","shell.execute_reply":"2022-03-28T07:01:40.017046Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":"### Summary","metadata":{"ExecuteTime":{"end_time":"2020-11-09T15:24:18.245064Z","start_time":"2020-11-09T15:24:18.241548Z"},"id":"e80d-5oiFm7e"}},{"cell_type":"markdown","source":"We observe the following:\n\n1. **Underfitting** (degree < 5): The model is not able to fit the data properly. The fit is bad for both the training and the validation set.\n\n2. **Fit is just right** (degree = 5): The model is able to capture the underlying data distribution. The fit is good for both the training and the validation set.\n\n3. **Overfitting** (degree > 5): The model starts fitting the noise in the dataset. While the fit for the training data gets even better, the fit for the validation set gets worse.\n\n4. As the order of polynomial increases, the linear model coefficients become more likely to take on **large values**.","metadata":{"id":"FIjJPBttFm7e"}},{"cell_type":"markdown","source":"## Part 2: Regularization","metadata":{"ExecuteTime":{"end_time":"2020-11-03T08:47:28.699189Z","start_time":"2020-11-03T08:47:28.695466Z"},"id":"1MT-68ERFm7f"}},{"cell_type":"markdown","source":"There are two major ways to build a machine learning model with the ability to generalize well on unseen data:\n1. Train the simplest model possible for our purpose (according to Occamâs Razor).\n2. Train a complex or more expressive model on the data and perform regularization.\n\nRegularization is a method used to reduce the variance of a machine learning model. In other words, it is used to reduce overfitting. Regularization penalizes a model for being complex. For linear models, it means regularization forces model coefficients to be smaller in magnitude.\n\nLet's pick a polynomial model of degree **15** (which tends to overfit strongly) and try to regularize it using **L1** and **L2** penalties.","metadata":{"id":"vLFkPt07Fm7f"}},{"cell_type":"markdown","source":"### L1 - Lasso regression","metadata":{"id":"Iy0SoHrQFm7g"}},{"cell_type":"code","source":"rmse_train, rmse_valid = [], []\nfor alpha in ALPHAS:    \n    results = cross_validate(make_model(degree=15, penalty='L1', alpha=alpha), \n                             X, y, cv=5,\n                             return_train_score=True,\n                             scoring='neg_root_mean_squared_error')\n    rmse_train.append(-np.mean(results['train_score']))\n    rmse_valid.append(-np.mean(results['test_score']))\n    \nplot_fitting_graph(ALPHAS, rmse_train, rmse_valid,\n                   xlabel='Regularization strength (alpha)', ylabel='Error (RMSE)',\n                   title='Lasso polynomial regression (L1): degree=15')","metadata":{"ExecuteTime":{"end_time":"2021-12-15T20:32:49.851006Z","start_time":"2021-12-15T20:32:48.876099Z"},"id":"P-XUj3bvFm7g","outputId":"5c62b1f7-c4f0-4384-e13e-8b4108962d22","execution":{"iopub.status.busy":"2022-03-28T07:01:40.019502Z","iopub.execute_input":"2022-03-28T07:01:40.019723Z","iopub.status.idle":"2022-03-28T07:01:41.328252Z","shell.execute_reply.started":"2022-03-28T07:01:40.019696Z","shell.execute_reply":"2022-03-28T07:01:41.327366Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":"### L2 - Ridge regression","metadata":{"id":"5BII4Nb5Fm7g"}},{"cell_type":"code","source":"rmse_train, rmse_valid = [], []\nfor alpha in ALPHAS:    \n    results = cross_validate(make_model(degree=15, penalty='L2', alpha=alpha), \n                             X, y, cv=5,\n                             return_train_score=True,\n                             scoring='neg_root_mean_squared_error')\n    rmse_train.append(-np.mean(results['train_score']))\n    rmse_valid.append(-np.mean(results['test_score']))\n    \nplot_fitting_graph(ALPHAS, rmse_train, rmse_valid, \n                   xlabel='Regularization strength (alpha)', ylabel='Error (RMSE)', \n                   title='Ridge polynomial regression (L2): degree=15')","metadata":{"ExecuteTime":{"end_time":"2021-12-15T20:32:51.202745Z","start_time":"2021-12-15T20:32:50.468099Z"},"id":"AH5Iht1TFm7h","outputId":"596c94a4-3ff6-4ffe-9116-cc308700d4b5","execution":{"iopub.status.busy":"2022-03-28T07:01:41.330269Z","iopub.execute_input":"2022-03-28T07:01:41.331450Z","iopub.status.idle":"2022-03-28T07:01:42.366112Z","shell.execute_reply.started":"2022-03-28T07:01:41.331379Z","shell.execute_reply":"2022-03-28T07:01:42.365283Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":"### Summary","metadata":{"ExecuteTime":{"end_time":"2020-11-09T13:47:39.048589Z","start_time":"2020-11-09T13:47:39.044912Z"},"id":"QlA4Poe5Fm7i"}},{"cell_type":"markdown","source":"1. We can control the regularization strength by changing the hyperparameter `alpha`.\n2. Regularized version of the model performs pretty well. Even in case the original original (unregularized) model is heavily overfitting due to excessive complexity.","metadata":{"ExecuteTime":{"end_time":"2020-11-09T13:49:03.993455Z","start_time":"2020-11-09T13:49:03.987472Z"},"id":"LN2iPlfqFm7i"}},{"cell_type":"markdown","source":"## Part 3: Homework assignment (10 points)","metadata":{"ExecuteTime":{"end_time":"2020-11-09T12:14:17.962945Z","start_time":"2020-11-09T12:14:17.959952Z"},"id":"4RV8l5_LFm7i"}},{"cell_type":"markdown","source":"### Excercise 1 - Overfiting and Underfitting (2 points)","metadata":{"ExecuteTime":{"end_time":"2021-12-10T12:27:23.202301Z","start_time":"2021-12-10T12:27:23.185315Z"},"id":"bEy14ckgFm7j"}},{"cell_type":"markdown","source":"Let's work with the diabetes dataset","metadata":{"ExecuteTime":{"end_time":"2021-12-10T07:35:07.485715Z","start_time":"2021-12-10T07:35:07.461799Z"},"id":"uVNhx7sgFm7j"}},{"cell_type":"code","source":"from sklearn.datasets import load_diabetes\ndata = load_diabetes()\nX_diabetes = pd.DataFrame(data['data'], columns=data['feature_names'])\ny_diabetes = pd.DataFrame(data['target'], columns=['target'])\nprint(data['DESCR'])","metadata":{"ExecuteTime":{"end_time":"2021-12-15T20:32:51.45077Z","start_time":"2021-12-15T20:32:51.234725Z"},"id":"pqzlaOg9Fm7j","execution":{"iopub.status.busy":"2022-03-28T07:01:42.367601Z","iopub.execute_input":"2022-03-28T07:01:42.367924Z","iopub.status.idle":"2022-03-28T07:01:42.465537Z","shell.execute_reply.started":"2022-03-28T07:01:42.367882Z","shell.execute_reply":"2022-03-28T07:01:42.464393Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"markdown","source":"Apply model for diabetes dataset with polynomial feature engineering of different degrees. Plot the dependence of train and test error on polynomial degree. Highlight a degree with the best test error. Which degrees cause overfitting/underfitting? Why?","metadata":{"id":"zLIqUoeCFm7k"}},{"cell_type":"code","source":"# Ð Ð°Ð·Ð´ÐµÐ»Ð¸Ð¼ Ð´Ð°Ð½Ð½ÑÐµ Ð½Ð° ÑÑÐµÐ½Ð¸ÑÐ¾Ð²Ð¾ÑÐ½ÑÐµ Ð¸ ÑÐµÑÑÐ¾Ð²ÑÐµ Ð² ÑÐ¾Ð¾ÑÐ½Ð¾ÑÐµÐ½Ð¸Ð¸ 70 Ðº 30\nprint(f'Ð Ð°Ð·Ð¼ÐµÑ Ð¸ÑÑÐ¾Ð´Ð½ÑÑ Ð´Ð°Ð½Ð½ÑÑ: {X_diabetes.shape, y_diabetes.shape}')\nX_train_diabetes, X_test_diabetes, y_train_diabetes, y_test_diabetes = train_test_split(X_diabetes, y_diabetes, test_size=0.3, random_state=SEED)\nprint(f'\\nÐ Ð°Ð·Ð¼ÐµÑ ÑÑÐµÐ½Ð¸ÑÐ¾Ð²Ð¾ÑÐ½ÑÑ Ð´Ð°Ð½Ð½ÑÑ: {X_train_diabetes.shape,y_train_diabetes.shape}')\nprint(f'Ð Ð°Ð·Ð¼ÐµÑ ÑÐµÑÑÐ¾Ð²ÑÑ Ð´Ð°Ð½Ð½ÑÑ:{X_test_diabetes.shape, y_test_diabetes.shape}')\n","metadata":{"id":"4YfFYWA_Fm7l","outputId":"25e9b8e3-2d6c-4ebc-c05d-1f473ac4a42f","execution":{"iopub.status.busy":"2022-03-28T07:03:07.924510Z","iopub.execute_input":"2022-03-28T07:03:07.925108Z","iopub.status.idle":"2022-03-28T07:03:07.932752Z","shell.execute_reply.started":"2022-03-28T07:03:07.925073Z","shell.execute_reply":"2022-03-28T07:03:07.931951Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ÐÐµÑÐµÐ¿Ð¸ÑÐµÐ¼ ÑÑÐ½ÐºÑÐ¸Ñ Ð¿Ð¾ÑÑÑÐ¾ÐµÐ½Ð¸Ñ Ð¼Ð¾Ð´ÐµÐ»Ð¸ ( Ð²ÑÐ±Ð¾Ñ ÐºÐ¾ÑÑÐ¸ÑÐ¸ÐµÐ½ÑÐ° Ð¸ Ð¼ÐµÑÐ¾Ð´Ð° ÑÐµÐ³ÑÐ»ÑÑÐ¸Ð·Ð°ÑÐ¸Ð¸ Ð½Ð°Ð¼ Ð¿Ð¾Ð½Ð°Ð´Ð¾Ð±Ð¸ÑÑÑ Ð´Ð»Ñ ÑÐ»ÐµÐ´ÑÑÑÐ¸Ñ ÑÐ°ÑÐºÐ¾Ð² )\n# ÑÐ°Ðº ÐºÐ°Ðº Ð¼Ð¾ÑÐ½Ð¾ÑÑÐµÐ¹ Ð½Ðµ ÑÐ²Ð°ÑÐ°ÐµÑ Ð¾Ð³ÑÐ°Ð½Ð¸ÑÐ¸Ð¼ Ð¿Ð¾ÑÑÑÐ¾Ð½ÐµÐ½Ð¸Ðµ Ð´Ð»Ñ 3 ÑÐ¸Ñ(age,sex,bmi) Ñ Ð¿Ð¾Ð¼Ð¾ÑÑÑ ColumnTransformer\nfrom sklearn.compose import ColumnTransformer\n# Ð¸ ÑÐ°Ðº ÐºÐ°Ðº Ð´Ð°Ð½Ð½ÑÐµ Ð¿Ð¾ ÑÑÐ»Ð¾Ð²Ð¸Ñ ÑÐ°ÐºÐ¸Ð»Ð¸ÑÐ¾Ð²Ð°Ð½Ñ, ÑÐ±ÐµÑÐµÐ¼ ÑÐºÐ°Ð»Ð¸ÑÐ¾Ð²Ð°Ð½Ð¸Ðµ\ndef make_modelD(degree,  alpha=0, penalty=None):\n    # linear regression\n    if alpha == 0:\n        regressor = LinearRegression()\n    # lasso regression\",\n    elif penalty == 'L1':\n        regressor = Lasso(alpha=alpha, random_state=SEED, max_iter=100000)\n    # ridge regression\",\n    elif penalty == 'L2':\n        regressor = Ridge(alpha=alpha, random_state=SEED, max_iter=100000)\n    \n    polFiltred = ColumnTransformer([('PolynomialFeatures', PolynomialFeatures(degree, include_bias=(degree == 0)), ['age','sex','bmi'])], remainder='passthrough')\n\n    return Pipeline([\n        ('pol', polFiltred),\n       # ('sca', StandardScaler()),\n        ('reg', regressor)\n    ])","metadata":{"id":"fA-rygEaFm7l","execution":{"iopub.status.busy":"2022-03-28T07:13:26.951427Z","iopub.execute_input":"2022-03-28T07:13:26.952587Z","iopub.status.idle":"2022-03-28T07:13:26.959250Z","shell.execute_reply.started":"2022-03-28T07:13:26.952543Z","shell.execute_reply":"2022-03-28T07:13:26.958464Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"# ÐÐµÐ¼Ð½Ð¾Ð³Ð¾ ÑÐºÐ¾ÑÑÐµÐºÑÐ¸ÑÐ¾Ð²Ð°Ð»Ð° Ð¿Ð¾ÑÑÑÐ¾ÐµÐ½Ð¸Ðµ Ð³ÑÐ°ÑÐ¸ÐºÐ° Ð´Ð»Ñ Ð½Ð°Ð³Ð»ÑÐ´Ð½Ð¾ÑÑÐ¸ Ð¿Ð¾Ð»ÑÑÐ°ÐµÐ¼ÑÑ Ð·Ð½Ð°ÑÐµÐ½Ð¸Ð¹\ndef rmse(y_true, y_pred):\n    return np.sqrt(mean_squared_error(y_true, y_pred))\n\ndef plot_fitting_graphD(x, metric_train, metric_valid, xlabel, ylabel, \n                       custom_metric=None, custom_label='', custom_scale='log', title='Fitting graph'):\n    plt.figure(figsize=(9, 4.5))\n    plt.plot(x, metric_train, label='Training')\n    plt.plot(x, metric_valid, color='C1', label='Test')\n    plt.axvline(x[np.argmin(metric_train)], color='C2', lw=10, alpha=0.2)\n    plt.axvline(x[np.argmin(metric_valid)], color='C1', lw=10, alpha=0.2)\n\n    plt.title(title)\n    plt.xlabel(xlabel)\n    plt.ylabel(ylabel)\n    plt.grid(True)\n    plt.xticks(x, rotation='vertical')\n    plt.legend(loc='center left') \n    plt.yscale(custom_scale)\n    if custom_metric:\n        plt.twinx()\n        plt.yscale(custom_scale)\n        plt.plot(x, custom_metric, alpha=0.2, lw=4, ls='dotted', color='black', label=custom_label) \n        plt.legend(loc='center right')         \n    plt.show()","metadata":{"id":"7PBMMOjlFm7m","execution":{"iopub.status.busy":"2022-03-28T07:13:29.262182Z","iopub.execute_input":"2022-03-28T07:13:29.262971Z","iopub.status.idle":"2022-03-28T07:13:29.272895Z","shell.execute_reply.started":"2022-03-28T07:13:29.262922Z","shell.execute_reply":"2022-03-28T07:13:29.271856Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"code","source":"# ÐÐ¾ÑÐ¼Ð¾ÑÑÐ¸Ð¼ Ð²Ð»Ð¸ÑÐ½Ð¸Ðµ Ð¾ÑÐ¸Ð±ÐºÐ¸ Ð½Ð° ÑÑÐµÐ¿ÐµÐ½Ñ\nDEGREES = np.linspace(1, 10, 0 + 10, dtype=int)\n\nrmse_train, rmse_test, avg_coef = [], [], []\nfor degree in DEGREES:\n    reg = make_modelD(degree).fit(X_train_diabetes, y_train_diabetes)\n    rmse_train.append(rmse(reg.predict(X_train_diabetes), y_train_diabetes))\n    rmse_test.append(rmse(reg.predict(X_test_diabetes), y_test_diabetes))\n    avg_coef.append(np.mean(np.abs(reg['reg'].coef_)))\nplot_fitting_graphD(DEGREES, rmse_train, rmse_test, xlabel='Complexity (degree)', ylabel='Error (RMSE)', custom_metric=avg_coef, custom_label='avg(|$w_i$|)', \n                   title='Least squares polynomial regression')\n\n\n\nprint(f'ÐÐ¿ÑÐ¸Ð¼Ð°Ð»ÑÐ½Ð°Ñ ÑÑÐµÐ¿ÐµÐ½Ñ Ð½Ð° ÑÑÐµÐ½Ð¸ÑÐ¾Ð²Ð¾ÑÐ½ÑÑ Ð´Ð°Ð½Ð½ÑÑ: {DEGREES[np.argmin(rmse_train)]}')\nprint(f'ÐÐ¿ÑÐ¸Ð¼Ð°Ð»ÑÐ½Ð°Ñ ÑÑÐµÐ¿ÐµÐ½Ñ Ð½Ð° ÑÐµÑÑÐ¾Ð²ÑÑ Ð´Ð°Ð½Ð½ÑÑ: {DEGREES[np.argmin(rmse_test)]}')","metadata":{"id":"1V4kYrq1Fm7m","outputId":"812cd00c-c0c1-4937-897b-9d15976d6528","execution":{"iopub.status.busy":"2022-03-28T07:13:42.159722Z","iopub.execute_input":"2022-03-28T07:13:42.160033Z","iopub.status.idle":"2022-03-28T07:13:43.380581Z","shell.execute_reply.started":"2022-03-28T07:13:42.160003Z","shell.execute_reply":"2022-03-28T07:13:43.379543Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"code","source":"# ÐÐ¾Ð¿ÑÐ¾Ð±ÑÐµÐ¼ Ñ ÐºÑÐ¾ÑÑÐ²Ð°Ð»Ð¸Ð´Ð°ÑÐ¸ÐµÐ¹ ÑÐµ Ð¶Ðµ ÑÑÐµÐ¿ÐµÐ½Ð¸\nDEGREES = np.linspace(1, 10, 0 + 10, dtype=int)\n\nrmse_train, rmse_test, avg_coef = [], [], []\nfor degree in DEGREES:\n    results = cross_validate(make_modelD(degree),\n                             X_diabetes, y_diabetes, cv=5,\n                             return_train_score=True, return_estimator=True,\n                             scoring='neg_root_mean_squared_error')\n    rmse_train.append(-np.mean(results['train_score']))\n    rmse_test.append(-np.mean(results['test_score']))        \n    avg_coef.append(np.mean([np.mean(np.abs(model['reg'].coef_)) for model in results['estimator']]))\n \nplot_fitting_graphD(DEGREES, rmse_train, rmse_test,\n                   xlabel='Complexity (degree)', ylabel='Error (RMSE)',\n                   custom_metric=avg_coef, custom_label='avg(|$w_i$|)',\n                   title='Least squares polynomial regression')\n\nprint(f'ÐÐ¿ÑÐ¸Ð¼Ð°Ð»ÑÐ½Ð°Ñ ÑÑÐµÐ¿ÐµÐ½Ñ Ð½Ð° ÑÑÐµÐ½Ð¸ÑÐ¾Ð²Ð¾ÑÐ½ÑÑ Ð´Ð°Ð½Ð½ÑÑ: {DEGREES[np.argmin(rmse_train)]}')\nprint(f'ÐÐ¿ÑÐ¸Ð¼Ð°Ð»ÑÐ½Ð°Ñ ÑÑÐµÐ¿ÐµÐ½Ñ Ð½Ð° ÑÐµÑÑÐ¾Ð²ÑÑ Ð´Ð°Ð½Ð½ÑÑ: {DEGREES[np.argmin(rmse_test)]}')\n","metadata":{"id":"bOr8iw2_Fm7n","outputId":"935070e4-e3ca-4117-f132-200cbf17b54d","execution":{"iopub.status.busy":"2022-03-28T07:24:05.069262Z","iopub.execute_input":"2022-03-28T07:24:05.069566Z","iopub.status.idle":"2022-03-28T07:24:08.118332Z","shell.execute_reply.started":"2022-03-28T07:24:05.069535Z","shell.execute_reply":"2022-03-28T07:24:08.117381Z"},"trusted":true},"execution_count":50,"outputs":[]},{"cell_type":"markdown","source":"ÐÑÐ²Ð¾Ð´:\n\nÐÐ¸Ð½Ð¸Ð¼Ð°Ð»ÑÐ½Ð°Ñ Ð¾ÑÐ¸Ð±ÐºÐ° Ð½Ð° ÑÐµÑÑÐ¾Ð²ÑÑ Ð´Ð°Ð½Ð½ÑÑ Ð½Ð°Ð±Ð»ÑÐ´Ð°ÐµÑÑÑ Ð½Ð° ÑÑÐµÐ¿ÐµÐ½Ð¸ (degree = 2). ÐÑÐ¸ ÑÑÐµÐ¿ÐµÐ½Ð¸ 1 Ð²ÐµÐ»Ð¸ÐºÐ° Ð²ÐµÑÐ¾ÑÑÐ½Ð¾ÑÑÑ Ð½ÐµÐ´Ð¾ÑÑÐµÐ½Ð½Ð¾ÑÑÐ¸ Ð¼Ð¾Ð´ÐµÐ»Ð¸. ÐÑÐ¸ ÑÑÐµÐ¿ÐµÐ½Ð¸ >2 Ð½Ð°Ð±Ð»ÑÐ´Ð°ÐµÑÑÑ Ð·Ð½Ð°ÑÐ¸ÑÐµÐ»ÑÐ½ÑÐ¹ ÑÐ¾ÑÑ Ð¾ÑÐ¸Ð±ÐºÐ¸ rmse. ÐÐ½Ð°ÑÐ¸Ñ Ð²ÑÐ±Ð¾Ñ ÑÑÐµÐ¿ÐµÐ½Ð¸ > 2(Ð¾ÑÐ¾Ð±ÐµÐ½Ð½Ð¾ 4,5,8-10 Ñ.Ðº. Ð½Ð°Ð±Ð»ÑÐ´Ð°ÐµÑÑÑ Ð²ÑÐ±ÑÐ¾ÑÑ Ð²ÐµÑÐ¾Ð² ÐºÐ¾ÑÑÑÐ¸ÑÐ¸ÐµÐ½ÑÐ¾Ð²) Ð¼Ð¾Ð¶ÐµÑ Ð¿ÑÐ¸Ð²ÐµÑÑÐ¸ Ðº Ð¿ÐµÑÐµÐ¾Ð±ÑÑÐµÐ½Ð½Ð¾ÑÑÐ¸ Ð¼Ð¾Ð´ÐµÐ»Ð¸. ÐÑÐ¾Ð±ÐµÐ½Ð½Ð¾  \nÐÑÐ°ÑÐ¸Ðº Ð¸ Ð·Ð½Ð°ÑÐµÐ½Ð¸Ñ Ð¾ÑÐ¸Ð±Ð¾Ðº Ð´Ð°Ð¶Ðµ Ð½Ð° Ð¾Ð¿ÑÐ¸Ð¼Ð°Ð»ÑÐ½ÑÑ ÑÑÐµÐ¿ÐµÐ½ÑÑ ÑÐºÐ¾ÑÐµÐµ Ð³Ð¾Ð²Ð¾ÑÐ¸Ñ Ð¾ Ð½ÐµÑÐ²Ð°ÑÐºÐµ Ð¸ÑÑÐ¾Ð´Ð½ÑÑ Ð´Ð°Ð½Ð½ÑÑ Ð´Ð»Ñ Ð¾Ð±ÑÑÐµÐ½Ð¸Ñ, Ð¿ÑÐ¸ÑÐµÐ¼ ÐºÑÐ¾ÑÑÐ°Ð²Ð¸Ð´Ð°ÑÐ¸Ñ Ð½Ðµ ÑÐ¸Ð»ÑÐ½Ð¾ Ð¿Ð¾Ð¼Ð¾Ð³Ð»Ð°.  ","metadata":{"id":"RwLP_je6Fm7n"}},{"cell_type":"markdown","source":"### Excercise 2 - Magnitude (3 points)","metadata":{"ExecuteTime":{"end_time":"2021-12-10T12:46:46.756169Z","start_time":"2021-12-10T12:44:13.217Z"},"id":"UA4pLbxKFm7o"}},{"cell_type":"markdown","source":"As discussed earlier, regularization methods are expected to constraint the weights (model coefficients). \n\nIs it indeed happening? \n\nPlease do a discovery on your own and find that out empirically (both for **L1** and **L2**). Let's use `degree=15` and `alpha` from `ALPHAS`.","metadata":{"id":"CcysIDYCFm7o"}},{"cell_type":"code","source":"def plot_fitting_graph_coef(x, metric, xlabel, ylabel, \n                       custom_metric=None, custom_label='', custom_scale='log', title='Fitting graph'):\n    plt.figure(figsize=(9, 4.5))\n    plt.plot(x, metric, label='Coefficient')\n    plt.title(title)\n    plt.xlabel(xlabel)\n    plt.ylabel(ylabel)\n    plt.grid(True)\n    plt.xticks(x, rotation='vertical')\n    plt.legend(loc='center left')    \n    plt.yscale(custom_scale)\n    plt.show()","metadata":{"id":"z-rtn1A9Fm7o","execution":{"iopub.status.busy":"2022-03-28T07:24:48.327643Z","iopub.execute_input":"2022-03-28T07:24:48.327981Z","iopub.status.idle":"2022-03-28T07:24:48.334609Z","shell.execute_reply.started":"2022-03-28T07:24:48.327948Z","shell.execute_reply":"2022-03-28T07:24:48.333824Z"},"trusted":true},"execution_count":51,"outputs":[]},{"cell_type":"markdown","source":"#### L1","metadata":{"id":"Azzj2aFDFm7p"}},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"degree_= 15\nALPHAS=np.linspace(0, 0.5, 1 + 40)\n# Ð§ÑÐ¾Ð±Ñ Ð½Ðµ Ð¿ÐµÑÐµÑÑÐ¸ÑÑÐ²Ð°ÑÑ Ð² 3 Ð·Ð°Ð´Ð°Ð½Ð¸Ð¸, ÑÑÐ°Ð·Ñ ÑÐ¾ÑÑÐ°Ð½Ð¸Ð¼ ÐºÐ¾Ð»Ð¸ÑÐµÑÑÐ²Ð¾ Ð¾Ð±Ð½ÑÐ»ÐµÐ½Ð½ÑÑ ÐºÐ¾ÐµÑÑÐ¸ÑÐ¸ÐµÐ½ÑÐ¾Ð² \nzeroes_amount_L1 = dict()\n\n# Ð Ð·Ð°Ð´Ð°Ð½Ð¸Ð¸ Ð¿Ð¾ÑÐ¼Ð¾ÑÑÐµÑÑ Ð·Ð°Ð²Ð¸ÑÐ¸Ð¼Ð¾ÑÑ ÐºÐ¾ÑÑÑÐ¸ÑÐ¸ÐµÐ½ÑÐ¾Ð² Ð¾Ñ alpha\navg_coef = []\n# ÐÐ¾ÑÐ¼Ð¾ÑÑÐ¸Ð¼ ÐºÐ°Ðº alpha Ð²Ð»Ð¸ÑÐµÑ Ð½Ð° Ð¾ÑÐ¸Ð±ÐºÑ Ð½Ð° ÑÑÐ½Ð¸ÑÐ¾Ð²Ð¾ÑÐ½ÑÑ Ð¸ ÑÐµÑÑÐ¾Ð²ÑÑ Ð´Ð°Ð½Ð½ÑÑ\nrmse_train, rmse_test= [], []\nfor alpha in ALPHAS:    \n    modelD_L1 = make_modelD(degree=degree_, penalty='L1', alpha=alpha).fit(X_train_diabetes, y_train_diabetes)\n    # ÑÑÐµÐ´Ð½Ð¸Ðµ Ð·Ð½Ð°ÑÐµÐ½Ð¸Ñ ÐºÐ¾ÐµÑÑÐ¸ÑÐ¸ÐµÐ½ÑÐ¾Ð²\n    avg_coef.append(np.mean(np.abs(modelD_L1['reg'].coef_)))\n    \n    # Ð¾ÑÐ¸Ð±ÐºÐ¸\n    rmse_train.append(rmse(modelD_L1.predict(X_train_diabetes), y_train_diabetes))\n    rmse_test.append(rmse(modelD_L1.predict(X_test_diabetes), y_test_diabetes))\n    \n    # Ð´Ð»Ñ ÑÑÐµÑÑÐµÐ³Ð¾ Ð·Ð°Ð´Ð°Ð½Ð¸Ñ\n    zeroes_amount_L1[alpha] = np.count_nonzero(modelD_L1['reg'].coef_==0)\n    \n                \nplot_fitting_graph_coef(ALPHAS, avg_coef, xlabel='Complexity (alpha)', ylabel='AVG_COEF (avg(|$w_i$|))', title = 'Lasso method'  )\nplot_fitting_graphD(ALPHAS, rmse_train, rmse_test,\n                   xlabel='Complexity (alpha)', ylabel='Error (RMSE)',\n                   custom_metric=avg_coef, custom_label='avg(|$w_i$|)',\n                   title='Lasso influence on prediction')\nprint(f'ÐÐ¿ÑÐ¸Ð¼Ð°Ð»ÑÐ½Ð¾Ðµ Ð·Ð½Ð°ÑÐµÐ½Ð¸Ðµ alpha Ð´Ð»Ñ ÑÑÐµÐ½Ð¸ÑÐ¾Ð²Ð¾ÑÐ½ÑÑ Ð´Ð°Ð½Ð½ÑÑ: {ALPHAS[np.argmin(rmse_train)]}')\nprint(f'ÐÐ¿ÑÐ¸Ð¼Ð°Ð»ÑÐ½Ð¾Ðµ Ð·Ð½Ð°ÑÐµÐ½Ð¸Ðµ alpha Ð´Ð»Ñ ÑÑÐµÐ½Ð¸ÑÐ¾Ð²Ð¾ÑÐ½ÑÑ Ð´Ð°Ð½Ð½ÑÑ: {ALPHAS[np.argmin(rmse_test)]}')\n\nprint(f'\\nÐÐ½Ð°ÑÐµÐ½Ð¸Ðµ Ð¾ÑÐ¸Ð±ÐºÐ¸ Ð½Ð° ÑÐµÑÑÐ¾Ð²ÑÑ Ð´Ð°Ð½Ð½ÑÑ Ð±ÐµÐ· Lasso: {rmse_test[0]}')\nprint(f'ÐÐ½Ð°ÑÐµÐ½Ð¸Ðµ Ð¾ÑÐ¸Ð±ÐºÐ¸ Ð½Ð° ÑÐµÑÑÐ¾Ð²ÑÑ Ð´Ð°Ð½Ð½ÑÑ Ð´Ð»Ñ Lasso: {np.argmin(rmse_test)}')","metadata":{"id":"GyYVbCGyFm7p","outputId":"8ed74d17-bcd6-4938-a2c0-72a08609f085","execution":{"iopub.status.busy":"2022-03-28T07:26:38.219569Z","iopub.execute_input":"2022-03-28T07:26:38.219874Z","iopub.status.idle":"2022-03-28T07:26:43.720871Z","shell.execute_reply.started":"2022-03-28T07:26:38.219842Z","shell.execute_reply":"2022-03-28T07:26:43.719932Z"},"trusted":true},"execution_count":53,"outputs":[]},{"cell_type":"markdown","source":"#### L2","metadata":{"id":"tCw9BYxbFm7q"}},{"cell_type":"code","source":"## ÐÑÐ¾Ð´ÐµÐ»Ð°ÐµÐ¼ ÑÐ¾Ð¶Ðµ ÑÐ°Ð¼Ð¾Ðµ Ð´Ð»Ñ L2\ndegree_= 15\nALPHAS=np.linspace(0, 0.5, 1 + 40)\n# ÐÐ»Ñ ÑÑÐµÑÑÐµÐ³Ð¾ Ð·Ð°Ð´Ð°Ð½Ð¸Ñ \nzeroes_amount_L2 = dict()\n\navg_coef = []\nrmse_train, rmse_test= [], []\n\nfor alpha in ALPHAS: \n    modelD_L2 = make_modelD(degree=degree_, penalty='L2', alpha=alpha).fit(X_train_diabetes, y_train_diabetes)\n    # ÑÑÐµÐ´Ð½Ð¸Ðµ Ð·Ð½Ð°ÑÐµÐ½Ð¸Ñ ÐºÐ¾ÐµÑÑÐ¸ÑÐ¸ÐµÐ½ÑÐ¾Ð²\n\n    avg_coef.append(np.mean(np.abs(modelD_L2['reg'].coef_)))\n    \n    # Ð¾ÑÐ¸Ð±ÐºÐ¸\n    rmse_train.append(rmse(modelD_L2.predict(X_train_diabetes), y_train_diabetes))\n    rmse_test.append(rmse(modelD_L2.predict(X_test_diabetes), y_test_diabetes))\n     \n    # Ð´Ð»Ñ ÑÑÐµÑÑÐµÐ³Ð¾ Ð·Ð°Ð´Ð°Ð½Ð¸Ñ\n    zeroes_amount_L2[alpha] = np.count_nonzero(modelD_L2['reg'].coef_==0)\n    \n                \nplot_fitting_graph_coef(ALPHAS, avg_coef, xlabel='Complexity (alpha)', ylabel='AVG_COEF (avg(|$w_i$|))',title = 'Ridge method' )\nplot_fitting_graphD(ALPHAS, rmse_train, rmse_test,\n                   xlabel='Complexity (alpha)', ylabel='Error (RMSE)',\n                   custom_metric=avg_coef, custom_label='avg(|$w_i$|)',\n                   title='Least squares polynomial regression')\nprint(f'ÐÐ¿ÑÐ¸Ð¼Ð°Ð»ÑÐ½Ð¾Ðµ Ð·Ð½Ð°ÑÐµÐ½Ð¸Ðµ alpha Ð´Ð»Ñ ÑÑÐµÐ½Ð¸ÑÐ¾Ð²Ð¾ÑÐ½ÑÑ Ð´Ð°Ð½Ð½ÑÑ: {ALPHAS[np.argmin(rmse_train)]}')\nprint(f'ÐÐ¿ÑÐ¸Ð¼Ð°Ð»ÑÐ½Ð¾Ðµ Ð·Ð½Ð°ÑÐµÐ½Ð¸Ðµ alpha Ð´Ð»Ñ ÑÑÐµÐ½Ð¸ÑÐ¾Ð²Ð¾ÑÐ½ÑÑ Ð´Ð°Ð½Ð½ÑÑ: {ALPHAS[np.argmin(rmse_test)]}')\n\nprint(f'\\nÐÐ½Ð°ÑÐµÐ½Ð¸Ðµ Ð¾ÑÐ¸Ð±ÐºÐ¸ Ð½Ð° ÑÐµÑÑÐ¾Ð²ÑÑ Ð´Ð°Ð½Ð½ÑÑ Ð±ÐµÐ· Ridge: {rmse_test[0]}')\nprint(f'ÐÐ½Ð°ÑÐµÐ½Ð¸Ðµ Ð¾ÑÐ¸Ð±ÐºÐ¸ Ð½Ð° ÑÐµÑÑÐ¾Ð²ÑÑ Ð´Ð°Ð½Ð½ÑÑ Ð´Ð»Ñ Ridge: {np.argmin(rmse_test)}')\n","metadata":{"id":"vqYOXRiFFm7q","outputId":"f26d4b62-cfc2-498d-b4c8-533795ee8675","execution":{"iopub.status.busy":"2022-03-28T07:27:25.641707Z","iopub.execute_input":"2022-03-28T07:27:25.641970Z","iopub.status.idle":"2022-03-28T07:27:31.095223Z","shell.execute_reply.started":"2022-03-28T07:27:25.641943Z","shell.execute_reply":"2022-03-28T07:27:31.094062Z"},"trusted":true},"execution_count":54,"outputs":[]},{"cell_type":"markdown","source":"#### Summary","metadata":{"id":"F0XMdnB-Fm7r"}},{"cell_type":"markdown","source":"ÐÐ°Ðº Ð¸ Ð¾Ð¶Ð¸Ð´Ð°Ð»Ð¾ÑÑ, Ñ ÑÐ²ÐµÐ»Ð¸ÑÐµÐ½Ð¸ÐµÐ¼ alpha ÑÐ¼ÐµÐ½ÑÑÐ°ÑÑÑÑ Ð²ÐµÑÐ° ÐºÐ¾ÑÑÑÐ¸ÑÐ¸ÐµÐ½ÑÐ¾Ð² Ð¼Ð¾Ð´ÐµÐ»Ð¸. \nÐÐµÑÐ¾Ð´Ñ ÑÐµÐ³ÑÐ»ÑÑÐ¸Ð·Ð°ÑÐ¸Ð¸ Lasso Ð¸ Ridge Ð¾Ð´Ð¸Ð½Ð°ÐºÐ¾Ð²Ð¾ ÑÐ¾ÑÐ¾ÑÐ¾ ÑÐµÐ±Ñ Ð¿ÑÐ¾ÑÐ²Ð¸Ð»Ð¸ Ð½Ð° ÑÐµÑÑÐ¸ÑÑÐµÐ¼ÑÑ Ð´Ð°ÑÐ°ÑÐµÑÐ°Ñ","metadata":{}},{"cell_type":"markdown","source":"### Excercise 3 - Sparsity (3 points)","metadata":{"ExecuteTime":{"end_time":"2021-12-10T12:46:46.756169Z","start_time":"2021-12-10T12:44:13.217Z"},"id":"9PNU0CEGFm7t"}},{"cell_type":"markdown","source":"Lasso can also be used for **feature selection** since L1 is [more likely to produce zero coefficients](https://explained.ai/regularization/).\n\nIs it indeed happening? \n\nPlease do a discovery on your own and find that out empirically (both for **L1** and **L2**). Let's use `degree=15` and `alpha` from `ALPHAS`.","metadata":{"id":"aE279b-VFm7t"}},{"cell_type":"markdown","source":"#### L1","metadata":{"id":"4vcWycZuFm7u"}},{"cell_type":"code","source":"## Ð¿Ð¾ÑÑÑÐ¾Ð¸Ð¼ Ð³ÑÐ°ÑÐ¸Ðº Ð·Ð°Ð²Ð¸ÑÐ¸Ð¼Ð¾ÑÑÐµÐ¹ ÐºÐ¾Ð»-Ð²Ð° Ð¾Ð±Ð½ÑÐ»ÐµÐ½Ð½ÑÑ ÐºÐ¾ÑÑÑÐ¸ÑÐ¸ÐµÐ½ÑÐ¾Ð² Ð¾Ñ alpha\nfig, ax = plt.subplots(figsize = (9,4.5))\n\ndt = pd.DataFrame(zeroes_amount_L1, index = zeroes_amount_L1.keys())\n\nplt.plot( zeroes_amount_L1.keys(), zeroes_amount_L1.values())\nplt.show()\n\n","metadata":{"id":"WWursO1jFm7u","execution":{"iopub.status.busy":"2022-03-28T07:32:47.870192Z","iopub.execute_input":"2022-03-28T07:32:47.870508Z","iopub.status.idle":"2022-03-28T07:32:48.056890Z","shell.execute_reply.started":"2022-03-28T07:32:47.870479Z","shell.execute_reply":"2022-03-28T07:32:48.056255Z"},"trusted":true},"execution_count":64,"outputs":[]},{"cell_type":"markdown","source":"#### L2","metadata":{"id":"Q5tok0faFm7u"}},{"cell_type":"code","source":"## Ð´Ð»Ñ Ð¼ÐµÑÐ¾Ð´Ð° Ridge Ð½ÑÐ»ÐµÐ²ÑÑ Ð½Ðµ Ð¿Ð¾Ð»ÑÑÐ¸Ð»Ð¾ÑÑ\nres = all(x == 0 for x in zeroes_amount_L2.values())\nprint(f'ÐÑÐ»ÐµÐ²ÑÐµ ÐºÐ¾ÐµÑÑÐ¸ÑÐ¸ÐµÐ½ÑÑ Ð¾ÑÑÑÑÑÑÐ²ÑÑÑ: {res}')","metadata":{"id":"Uvm1XlQGFm7v","execution":{"iopub.status.busy":"2022-03-28T07:28:03.937826Z","iopub.execute_input":"2022-03-28T07:28:03.938088Z","iopub.status.idle":"2022-03-28T07:28:03.943407Z","shell.execute_reply.started":"2022-03-28T07:28:03.938061Z","shell.execute_reply":"2022-03-28T07:28:03.942765Z"},"trusted":true},"execution_count":56,"outputs":[]},{"cell_type":"markdown","source":"#### Summary","metadata":{"id":"HoIs4QMZFm7v"}},{"cell_type":"markdown","source":"ÐÐ°Ðº Ð¸ Ð¾Ð¶Ð¸Ð´Ð°Ð»Ð¾ÑÑ, Ð¼ÐµÑÐ¾Ð´ Lasso Ð¿ÑÐ¸Ð²ÐµÐ» Ðº Ð¾Ð±Ð½ÑÐ»ÐµÐ½Ð¸Ñ ÐºÐ¾ÑÑÑÐ¸ÑÐ¸ÐµÐ½ÑÐ¾Ð²,Ð¿ÑÐ¸ÑÐµÐ¼ Ñ ÑÐ²ÐµÐ»Ð¸ÑÐµÐ½Ð¸ÐµÐ¼ alpha Ð±Ð¾Ð»ÑÑÐµ ÐºÐ¾ÑÑÑÐ¸ÑÐ¸ÐµÐ½ÑÐ¾Ð² Ð¾Ð±Ð½ÑÐ»ÑÐµÑÑÑ.\nÐ Ð¼ÐµÑÐ¾Ð´Ðµ Ridge Ð²ÐµÑ ÐºÐ¾ÑÑÑÐ¸ÑÐ¸ÐµÐ½ÑÐ¾Ð² ÑÐ¼ÐµÐ½ÑÑÐ°ÐµÑÑÑ Ñ ÑÐ²ÐµÐ»Ð¸ÑÐµÐ½Ð¸ÐµÐ¼ alpha, Ð¾Ð´Ð½Ð°ÐºÐ¾ Ð¾Ð±Ð½ÑÐ»ÐµÐ½Ð¸Ñ Ð½Ðµ Ð¿ÑÐ¾Ð¸ÑÑÐ¾Ð´Ð¸Ñ.","metadata":{}},{"cell_type":"markdown","source":"### Excercise 4 - Scaling (2 points)","metadata":{"id":"RKcMggDQFm7w"}},{"cell_type":"markdown","source":"As a general rule, it is recommended to scale input features before fitting a regularized model so that the features/inputs take values in similar ranges. One common way of doing so is to standardize the inputs and that is exactly what our pipeline  second step (`StandardScaler`) is responsible for. \n\nIs scaling important? What are the underlying reasons?\n\nPlease do a discovery on your own and find that out empirically (both for **L1** and **L2**) on the dataset below. Check coefficients.","metadata":{"id":"odvQdZTUFm7w"}},{"cell_type":"code","source":"def target_function_hw(x):\n    return 2 * x\n\ndef generate_samples_hw():\n    np.random.seed(SEED)\n    x = np.random.uniform(*RANGE, size=N_SAMPLES)\n    \n    np.random.seed(SEED+1)\n    x_noise = np.random.uniform(*[x * 100 for x in RANGE], size=N_SAMPLES)\n    x_noise2 = np.random.normal(100, 50, size=N_SAMPLES)\n    \n    y = target_function_hw(x) + np.random.normal(scale=4, size=N_SAMPLES)\n    \n    return np.concatenate([x.reshape(-1, 1) / 100, \n                           x_noise.reshape(-1, 1),\n                           x_noise2.reshape(-1, 1)], axis=1), y\n\nX_hw, y_hw = generate_samples_hw()\n\nfor i in range(X_hw.shape[1]):\n    print(f'Min of feature {i}: {min(X_hw[:, i]):.2f}, max: {max(X_hw[:, i]):.2f}')","metadata":{"id":"7JJlRDbDFm7x","outputId":"8227cc29-1bf5-4ab5-cd13-683e0ceabb31","execution":{"iopub.status.busy":"2022-03-28T07:33:43.440109Z","iopub.execute_input":"2022-03-28T07:33:43.440387Z","iopub.status.idle":"2022-03-28T07:33:43.450149Z","shell.execute_reply.started":"2022-03-28T07:33:43.440357Z","shell.execute_reply":"2022-03-28T07:33:43.449441Z"},"trusted":true},"execution_count":65,"outputs":[]},{"cell_type":"code","source":"# ÐÐ¾ÑÑÑÐ¾Ð¸Ð¼ Ð¼Ð¾Ð´ÐµÐ»Ñ Ð±ÐµÐ· ÑÐºÐ°Ð»Ð¸ÑÐ¾Ð²Ð°Ð½Ð¸Ñ\ndef make_model_Noscalling(degree,  alpha=0, penalty=None):\n    # linear regression\n    if alpha == 0:\n        regressor = LinearRegression()\n    # lasso regression\",\n    elif penalty == 'L1':\n        regressor = Lasso(alpha=alpha, random_state=SEED, max_iter=100000)\n    # ridge regression\",\n    elif penalty == 'L2':\n        regressor = Ridge(alpha=alpha, random_state=SEED, max_iter=100000) \n    \n    \n    return Pipeline([\n        ('pol', PolynomialFeatures(degree, include_bias=(degree == 0))),\n        \n        ('reg', regressor)\n    ])","metadata":{"execution":{"iopub.status.busy":"2022-03-28T07:34:49.513226Z","iopub.execute_input":"2022-03-28T07:34:49.513537Z","iopub.status.idle":"2022-03-28T07:34:49.519588Z","shell.execute_reply.started":"2022-03-28T07:34:49.513506Z","shell.execute_reply":"2022-03-28T07:34:49.518696Z"},"trusted":true},"execution_count":70,"outputs":[]},{"cell_type":"markdown","source":"#### L1","metadata":{"id":"Y89bxdD1Fm7x"}},{"cell_type":"code","source":"## your code\nDEGREES = np.linspace(1, 10, 0 + 10, dtype=int)\nrmse_train, rmse_valid= [], []\n# ÐÐ°Ð¹Ð´ÐµÐ¼ Ð¾Ð¿ÑÐ¸Ð¼Ð°Ð»ÑÐ½ÑÑ ÑÑÐµÐ¿ÐµÐ½Ñ\nfor degree in DEGREES:\n    results = cross_validate(make_model(degree),\n                             X_hw, y_hw, cv=5,\n                             return_train_score=True, return_estimator=True,\n                             scoring='neg_root_mean_squared_error') \n    rmse_train.append(-np.mean(results['train_score']))\n    rmse_valid.append(-np.mean(results['test_score']))\n\nprint(f'ÐÐ¿ÑÐ¸Ð¼Ð°Ð»ÑÐ½Ð°Ñ ÑÑÐµÐ¿ÐµÐ½Ñ Ð½Ð° ÑÐµÑÑÐ¾Ð²ÑÑ Ð´Ð°Ð½Ð½ÑÑ: {DEGREES[np.argmin(rmse_valid)]}') \ndegree_= DEGREES[np.argmin(rmse_valid)]\n","metadata":{"id":"8DQpRV0ZFm7y","execution":{"iopub.status.busy":"2022-03-28T07:34:25.498617Z","iopub.execute_input":"2022-03-28T07:34:25.498899Z","iopub.status.idle":"2022-03-28T07:34:25.790343Z","shell.execute_reply.started":"2022-03-28T07:34:25.498870Z","shell.execute_reply":"2022-03-28T07:34:25.788185Z"},"trusted":true},"execution_count":67,"outputs":[]},{"cell_type":"code","source":"# ÐÐ°Ð¹Ð´ÐµÐ¼ Ð¾Ð¿ÑÐ¸Ð¼Ð°Ð»ÑÐ½ÑÑ alpha\nALPHAS=np.linspace(0, 0.5, 1 + 40)\nrmse_valid = []\nfor alpha in ALPHAS:\n    results = cross_validate(make_model(degree=degree_, penalty='L1', alpha=alpha),\n                             X_hw, y_hw, cv=5,\n                             return_train_score=True, return_estimator=True,\n                             scoring='neg_root_mean_squared_error') \n    rmse_valid.append(-np.mean(results['test_score']))\n\nprint(f'ÐÐ¿ÑÐ¸Ð¼Ð°Ð»ÑÐ½Ð°Ñ alpha Ð½Ð° ÑÐµÑÑÐ¾Ð²ÑÑ Ð´Ð°Ð½Ð½ÑÑ: {ALPHAS[np.argmin(rmse_valid)]}') \nalpha_= ALPHAS[np.argmin(rmse_valid)]\n","metadata":{"execution":{"iopub.status.busy":"2022-03-28T07:33:46.288379Z","iopub.execute_input":"2022-03-28T07:33:46.288753Z","iopub.status.idle":"2022-03-28T07:34:12.938787Z","shell.execute_reply.started":"2022-03-28T07:33:46.288724Z","shell.execute_reply":"2022-03-28T07:34:12.937868Z"},"trusted":true},"execution_count":66,"outputs":[]},{"cell_type":"code","source":"X_hw_train, X_hw_test, y_hw_train, y_hw_test = train_test_split(X_hw, y_hw, test_size=0.3, random_state=SEED)\n\nmodel = make_model(degree=degree_,penalty='L1', alpha=alpha_).fit(X_hw_train, y_hw_train)\ny_pred_train = model.predict(X_hw_train)\ny_pred_test = model.predict(X_hw_test)\nrmse_train =rmse(y_pred_train,y_hw_train)\nrmse_test = rmse(y_pred_test,y_hw_test)\n\nprint(f'ÐÑÐ¸Ð±ÐºÐ° RMSE Ð´Ð»Ñ ÑÐºÐ°Ð»Ð¸ÑÐ¾Ð²Ð°Ð½Ð½ÑÑ Ð´Ð°Ð½Ð½ÑÑ Ñ ÑÐµÐ³ÑÐ»ÑÑÐ¸Ð·Ð°ÑÐ¸ÐµÐ¹ Lasso: {rmse_test}')","metadata":{"execution":{"iopub.status.busy":"2022-03-28T07:34:31.547884Z","iopub.execute_input":"2022-03-28T07:34:31.548174Z","iopub.status.idle":"2022-03-28T07:34:31.559567Z","shell.execute_reply.started":"2022-03-28T07:34:31.548143Z","shell.execute_reply":"2022-03-28T07:34:31.558524Z"},"trusted":true},"execution_count":68,"outputs":[]},{"cell_type":"code","source":"model = make_model_Noscalling(degree=degree_,penalty='L1', alpha=alpha_).fit(X_hw_train, y_hw_train)\ny_pred_train = model.predict(X_hw_train)\ny_pred_test = model.predict(X_hw_test)\nrmse_train =rmse(y_pred_train,y_hw_train)\nrmse_test = rmse(y_pred_test,y_hw_test)\n\nprint(f'ÐÑÐ¸Ð±ÐºÐ° RMSE Ð´Ð»Ñ Ð½ÐµÑÐºÐ°Ð»Ð¸ÑÐ¾Ð²Ð°Ð½Ð½ÑÑ Ð´Ð°Ð½Ð½ÑÑ Ñ ÑÐµÐ³ÑÐ»ÑÑÐ¸Ð·Ð°ÑÐ¸ÐµÐ¹ Lasso: {rmse_test}')","metadata":{"execution":{"iopub.status.busy":"2022-03-28T07:35:22.145134Z","iopub.execute_input":"2022-03-28T07:35:22.145445Z","iopub.status.idle":"2022-03-28T07:35:22.155768Z","shell.execute_reply.started":"2022-03-28T07:35:22.145402Z","shell.execute_reply":"2022-03-28T07:35:22.155046Z"},"trusted":true},"execution_count":71,"outputs":[]},{"cell_type":"markdown","source":"#### L2","metadata":{"id":"77Uv7hJ5Fm7y"}},{"cell_type":"code","source":"# ÐÐ°Ð¹Ð´ÐµÐ¼ Ð¾Ð¿ÑÐ¸Ð¼Ð°Ð»ÑÐ½ÑÑ alpha\nALPHAS=np.linspace(0, 0.5, 1 + 40)\nrmse_valid = []\nfor alpha in ALPHAS:\n    results = cross_validate(make_model(degree=degree_, penalty='L2', alpha=alpha),\n                             X_hw, y_hw, cv=5,\n                             return_train_score=True, return_estimator=True,\n                             scoring='neg_root_mean_squared_error') \n    rmse_valid.append(-np.mean(results['test_score']))\n\nprint(f'ÐÐ¿ÑÐ¸Ð¼Ð°Ð»ÑÐ½Ð°Ñ alpha Ð½Ð° ÑÐµÑÑÐ¾Ð²ÑÑ Ð´Ð°Ð½Ð½ÑÑ: {ALPHAS[np.argmin(rmse_valid)]}') \nalpha_= ALPHAS[np.argmin(rmse_valid)]","metadata":{"id":"9Lt-zPk5Fm7z","execution":{"iopub.status.busy":"2022-03-28T07:35:28.583983Z","iopub.execute_input":"2022-03-28T07:35:28.584716Z","iopub.status.idle":"2022-03-28T07:35:29.222262Z","shell.execute_reply.started":"2022-03-28T07:35:28.584675Z","shell.execute_reply":"2022-03-28T07:35:29.220980Z"},"trusted":true},"execution_count":72,"outputs":[]},{"cell_type":"code","source":"model = make_model(degree=degree_,penalty='L2', alpha=alpha_).fit(X_hw_train, y_hw_train)\ny_pred_train = model.predict(X_hw_train)\ny_pred_test = model.predict(X_hw_test)\nrmse_train =rmse(y_pred_train,y_hw_train)\nrmse_test = rmse(y_pred_test,y_hw_test)\n\nprint(f'ÐÑÐ¸Ð±ÐºÐ° RMSE Ð´Ð»Ñ ÑÐºÐ°Ð»Ð¸ÑÐ¾Ð²Ð°Ð½Ð½ÑÑ Ð´Ð°Ð½Ð½ÑÑ Ñ ÑÐµÐ³ÑÐ»ÑÑÐ¸Ð·Ð°ÑÐ¸ÐµÐ¹ Ridge: {rmse_test}')\n\nmodel = make_model_Noscalling(degree=degree_,penalty='L2', alpha=alpha_).fit(X_hw_train, y_hw_train)\ny_pred_train = model.predict(X_hw_train)\ny_pred_test = model.predict(X_hw_test)\nrmse_train =rmse(y_pred_train,y_hw_train)\nrmse_test = rmse(y_pred_test,y_hw_test)\n\nprint(f'ÐÑÐ¸Ð±ÐºÐ° RMSE Ð´Ð»Ñ Ð½ÐµÑÐºÐ°Ð»Ð¸ÑÐ¾Ð²Ð°Ð½Ð½ÑÑ Ð´Ð°Ð½Ð½ÑÑ Ñ ÑÐµÐ³ÑÐ»ÑÑÐ¸Ð·Ð°ÑÐ¸ÐµÐ¹ Ridge: {rmse_test}')\n","metadata":{"execution":{"iopub.status.busy":"2022-03-28T07:35:40.841442Z","iopub.execute_input":"2022-03-28T07:35:40.842048Z","iopub.status.idle":"2022-03-28T07:35:40.854842Z","shell.execute_reply.started":"2022-03-28T07:35:40.841995Z","shell.execute_reply":"2022-03-28T07:35:40.854138Z"},"trusted":true},"execution_count":73,"outputs":[]},{"cell_type":"markdown","source":"Ð Ð¿ÑÐ¾Ð²ÐµÐ´ÐµÐ½Ð½Ð¾Ð¼ ÑÐºÐ¿ÐµÑÐ¸Ð¼ÐµÐ½ÑÐµ Ð¼Ñ ÑÐ±ÐµÐ´Ð¸Ð»Ð¸ÑÑ, ÑÑÐ¾ ÑÐºÐ°Ð½Ð¸ÑÐ¾Ð²Ð°Ð½Ð¸Ðµ Ð´Ð°Ð½Ð½ÑÑ Ð¿ÑÐ¸Ð²Ð¾Ð´Ð¸Ñ ÑÐ¼ÐµÐ½ÑÑÐµÐ½Ð¸Ñ Ð¾ÑÐ¸Ð±ÐºÐ¸. Ð ÑÐ°ÐºÐ¶Ðµ, ÑÑÐ¾ Ð´Ð»Ñ Ð´Ð°Ð½Ð½ÑÑ Ð´Ð°Ð½Ð½ÑÑ Ð¿ÑÐµÐ´Ð¿Ð¾ÑÑÐ¸ÑÐµÐ»ÑÐ½ÐµÐ¹ ÑÑÑÐ¾Ð¸ÑÑ Ð¼Ð¾Ð´ÐµÐ»Ñ Ñ Ð¿ÑÐ¸Ð¼ÐµÐ½ÐµÐ½Ð¸ÐµÐ¼ Ð¼ÐµÑÐ¾Ð´Ð° Lasso.","metadata":{}}]}